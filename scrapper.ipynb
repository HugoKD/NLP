{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fde5208",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "988ecfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "from serpapi import GoogleSearch as search\n",
    "from tqdm import tqdm\n",
    "from timeout_decorator import timeout\n",
    "import json\n",
    "import shutil\n",
    "import os \n",
    "import pandas as pd\n",
    "from goose3 import Goose\n",
    "import ssl\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from serpapi import GoogleSearch\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "651507ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lacunaire = pd.read_excel('/Users/hugo.cadet/PycharmProjects/Meltwater_tag.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1778719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/Users/hugo.cadet/Downloads/Copie de 20241202_Mohammed_bin_Salman_Meltwater data excluding Twitter.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c522fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(data_lacunaire[['Document ID','Scrapped']], on='Document ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc179d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ini = data.copy()\n",
    "data = data.dropna(subset=['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9ea1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_false = data[data['Scrapped'] == False]\n",
    "reach_by_domain = data_false.groupby('Source Domain')['Reach'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48cbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true = data[data['Scrapped'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e036759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (data_false.groupby('Source Domain').size() / len(data_false)).reset_index()\n",
    "p1.columns = ['Source Domain', 'Share of Unscraped Articles per Source Among All Unscraped Articles']\n",
    "\n",
    "p1['Number of Unscraped Articles'] = data_false['Source Domain'].value_counts().reindex(p1['Source Domain']).values\n",
    "\n",
    "reach_per_source = (\n",
    "    data_false.groupby('Source Domain')['Reach']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'Reach': 'reach'})\n",
    ")\n",
    "p1 = p1.merge(reach_per_source, on='Source Domain', how='left')\n",
    "\n",
    "false_counts = data_false['Source Domain'].value_counts()\n",
    "total_counts = data['Source Domain'].value_counts()\n",
    "proportion_total = (false_counts / total_counts).reset_index()\n",
    "proportion_total.columns = ['Source Domain', 'Unscraped Articles Share per Source']\n",
    "\n",
    "excel_final = p1.merge(proportion_total, on='Source Domain', how='left')\n",
    "\n",
    "excel_final = excel_final[\n",
    "    [\n",
    "        'Source Domain',\n",
    "        'reach',\n",
    "        'Number of Unscraped Articles',\n",
    "        'Share of Unscraped Articles per Source Among All Unscraped Articles',\n",
    "        'Unscraped Articles Share per Source'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca04fb",
   "metadata": {},
   "source": [
    "### msn.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd1c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msn = data_false[data_false['Source Domain'] == 'msn.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1fedff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Final Version ####\n",
    "## goose non possible\n",
    "# Obligé de récup l'id dans l'url puis de l'utiliser dans l'API\n",
    "\n",
    "def get_ids(msn_urls): \n",
    "    ids = []\n",
    "    for url in msn_urls:\n",
    "        url_parts = str(url).split(\"-\")  \n",
    "        last_part = url_parts[-1]\n",
    "        if len(last_part) == 8:\n",
    "            ids.append(last_part)\n",
    "        elif len(last_part) == 17:\n",
    "            ids.append(last_part[:-9])  # Enlève le suffixe \"#comment\"\n",
    "        elif len(last_part) == 5 and len(url_parts) >= 3:\n",
    "            ids.append(url_parts[-3][:8])\n",
    "        else:\n",
    "            ids.append(None)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def msn_scraper(msn, is_url=False, urls=None):\n",
    "    contents = []\n",
    "    raw_content = []\n",
    "\n",
    "    if is_url:  \n",
    "        urls = urls\n",
    "    else: \n",
    "        urls = msn['URL'].tolist()\n",
    "\n",
    "    ids = get_ids(urls)\n",
    "\n",
    "    for i, id_ in tqdm(enumerate(ids), desc='Processing msn URLs'):\n",
    "        if id_ is None:\n",
    "            print(f\"ID manquant pour URL : {urls[i]}\")\n",
    "            row_id = msn['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                \"row_id\": row_id,      \n",
    "                'data': {\n",
    "                    'meta': {'canonical': urls[i]},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "            raw_content.append(None)\n",
    "            continue\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "            'Referer': 'https://www.msn.com/',\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Faire une requête pour récupérer les données de l'API\n",
    "            response = requests.get(\n",
    "                f'https://assets.msn.com/content/view/v2/Detail/en-us/{id_}', \n",
    "                headers=headers, timeout=10\n",
    "            )\n",
    "            raw_content.append(response.text)\n",
    "            response.raise_for_status()  # Vérifie si la requête a réussi (status code 200)\n",
    "\n",
    "            # Extraire les données JSON et le contenu HTML\n",
    "            data = response.json()\n",
    "            title = data.get('title', '')\n",
    "            body_html = data.get('body', '')\n",
    "            \n",
    "            # Utilisation de BeautifulSoup pour extraire le texte\n",
    "            soup = BeautifulSoup(body_html, \"html.parser\")\n",
    "            content_text = soup.get_text(separator=\"\\n\")  # Récupérer le texte\n",
    "\n",
    "            # Ajouter les données extraites dans le résultat\n",
    "            row_id = msn['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                \"row_id\": row_id,      \n",
    "                'data': {\n",
    "                    'meta': {'canonical': urls[i]},\n",
    "                    'image': None,  # Vous pouvez ajouter une logique pour récupérer une image si nécessaire\n",
    "                    'domain': None,  # Vous pouvez extraire le domaine si nécessaire\n",
    "                    'title': title,\n",
    "                    'cleaned_text': content_text,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Vous pouvez extraire la date de publication si nécessaire\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur pour l'ID {id_} : {e}\")\n",
    "            # En cas d'erreur, ajouter un article avec des valeurs par défaut\n",
    "            row_id = msn['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                \"row_id\": row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': urls[i]},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "\n",
    "        # Ajout d'un délai pour éviter de surcharger le serveur\n",
    "        time.sleep(0.5)  \n",
    "\n",
    "    return contents, raw_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2445f31",
   "metadata": {},
   "source": [
    "### Sauress.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "025693c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sauress = data_false[data_false['Source Domain'] == 'sauress.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b5a7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sauress_scraper(sauress):\n",
    "    contents_sauress, raw_content = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(sauress['URL']), total=len(sauress['URL']), desc=\"Processing sauress URLs\"):\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "                'Referer': 'https://www.msn.com/',\n",
    "            }\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            raw_content.append(response.text)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                # Vérification du titre de la page\n",
    "                title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
    "                if not title:\n",
    "                    og_title = soup.find(\"meta\", property=\"og:title\")\n",
    "                    if og_title and og_title.get(\"content\"):\n",
    "                        title = og_title[\"content\"].strip()  \n",
    "            else:\n",
    "                print(\"Page non disponible ou code HTTP différent de 200\")\n",
    "                title = \"Titre non disponible\"\n",
    "            \n",
    "            # Extraction du texte principal\n",
    "            spans = soup.find_all(\"span\", class_=\"articlecontent\")\n",
    "            row_id = sauress['Document ID'].iloc[i]\n",
    "            \n",
    "            # Construction des données de l'article\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,  # Ajouter la logique pour récupérer l'image si nécessaire\n",
    "                    'domain': None,  # Ajouter la logique pour récupérer le domaine si nécessaire\n",
    "                    'title': title,\n",
    "                    'cleaned_text': ', '.join([span.text.strip() for span in spans]),\n",
    "                    'opengraph': {},  # Ajouter les données OpenGraph si nécessaire\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Ajouter la logique pour récupérer la date de publication si nécessaire\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents_sauress.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n🔗 {url}\")\n",
    "            print(f\"Erreur de requête : {e}\")\n",
    "            # Ajouter des données par défaut en cas d'erreur\n",
    "            row_id = sauress['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents_sauress.append(article_data)\n",
    "\n",
    "    return contents_sauress, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb47e37",
   "metadata": {},
   "source": [
    "### shafaqna.com\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "451af5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, scraper l'article avant la div latest news, si il n'y a pas d'article avant cela veut dire qu'il n'est plus dispo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a6470e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://egypt.shafaqna.com/AR/AL/',\n",
       "       'https://qatar.shafaqna.com/AR/AL/',\n",
       "       'http://malaysia.shafaqna.com/MY/AL/',\n",
       "       'https://algeria.shafaqna.com/', 'https://uae.shafaqna.com/AR/AL/',\n",
       "       'https://indonesia.shafaqna.com/ID/AL/',\n",
       "       'http://kenya.shafaqna.com/EN/A',\n",
       "       'https://thailand.shafaqna.com/TH/AL/',\n",
       "       'http://zimbabwe.shafaqna.com/EN/AL/',\n",
       "       'https://venezuela.shafaqna.com/ES/AL/',\n",
       "       'http://bahrain.shafaqna.com/AR/AL',\n",
       "       'http://poland.shafaqna.com/PL/AL/',\n",
       "       'http://taiwan.shafaqna.com/CN/AL/',\n",
       "       'http://chile.shafaqna.com/ES/AL/',\n",
       "       'https://www.pk.shafaqna.com/EN/AL/',\n",
       "       'https://singapore.shafaqna.com/EN/AL/',\n",
       "       'https://fa.shafaqna.com/news/1731394/%d8%b9%d8%b1%d8%a8%d8%b3%d8%aa%d8%a7%d9%86-%d8%a8%db%8c%d8%b4-%d8%a7%d8%b2-%db%b8%db%b5-%d8%af%d8%b1%d8%b5%d8%af-%d8%a7%d8%b2-%d8%aa%d9%88%d8%b5%db%8c%d9%87-%d9%87%d8%a7%db%8c-%d8%b3',\n",
       "       'https://algeria.shafaqna.com/AR/AL/',\n",
       "       'http://malaysia.shafaqna.com/EN/AL/'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shafaqna = data_false[data_false['Source Domain'] == 'shafaqna.com']\n",
    "shafaqna['URL'].apply(lambda x : x[:-7]).unique()\n",
    "#shafaqna['URL'].apply(lambda x : x[-7:]).tolist()\n",
    "# Meme format : 'https://egypt.shafaqna.com/../ + int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc2e2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shafaqna_scraper(shafaqna):\n",
    "    content, raw_content = [], []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "        'Referer': 'https://www.msn.com/',\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(shafaqna['URL']), desc='Processing shafaqna URLs'):\n",
    "        row_id = shafaqna['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, verify=False, headers=headers)\n",
    "            raw_content.append(response.text)\n",
    "\n",
    "            # Vérification du code de statut\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Page non disponible ou code HTTP différent de 200 : {url}\")\n",
    "                \n",
    "                content.append(article_none)\n",
    "                continue\n",
    "\n",
    "            # Extraire le contenu principal de la page\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            news = soup.find(\"div\", id=\"right-column\")\n",
    "            news = news.find(\"div\", id=\"news-text\") if news else None\n",
    "\n",
    "            if not news:\n",
    "                print(f\"No news found in the url number {i} : {url}\")\n",
    "                \n",
    "                content.append(article_none)\n",
    "                continue\n",
    "\n",
    "            first_non_newsbox_div = news.find(\n",
    "                lambda tag: (\n",
    "                    tag.name == \"div\"\n",
    "                    and tag.get(\"class\") != [\"news\", \"box\"]\n",
    "                    and tag.get(\"id\") != \"titlebar\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if first_non_newsbox_div:\n",
    "                href = first_non_newsbox_div.find('a')['href']\n",
    "\n",
    "                # Faire une requête pour l'article lié\n",
    "                article_html = requests.get(href, timeout=10).text\n",
    "\n",
    "                # Extraire les données de l'article avec Goose\n",
    "                g = Goose()\n",
    "                article = g.extract(raw_html=article_html)\n",
    "\n",
    "                row_id = shafaqna['Document ID'].iloc[i]\n",
    "                article_data = {\n",
    "                    'row_id': row_id,\n",
    "                    'data': {\n",
    "                        'meta': {'meta_description': article.meta_description, 'url': url},\n",
    "                        'image': article.top_image.src if article.top_image else None,\n",
    "                        'domain': article.domain,\n",
    "                        'title': article.title,\n",
    "                        'cleaned_text': article.cleaned_text,\n",
    "                        'opengraph': article.opengraph,\n",
    "                        'tags': article.tags,\n",
    "                        'tweets': article.tweets,\n",
    "                        'movies': article.movies,\n",
    "                        'links': article.links,\n",
    "                        'authors': article.authors,\n",
    "                        'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                    }\n",
    "                }\n",
    "                content.append(article_data)\n",
    "            else:\n",
    "                print(f\"No suitable content found in the url number {i} : {url}\")\n",
    "                \n",
    "                content.append(article_none)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur pour {url}: {e}\")\n",
    "            # En cas d'erreur de requête, ajouter un article vide\n",
    "            \n",
    "            content.append(article_none)\n",
    "\n",
    "        # Délai pour éviter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return content, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a39550",
   "metadata": {},
   "source": [
    "### Alriyadh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d389e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url_no_verify(url: str):\n",
    "    \"\"\"Ouvre une URL sans vérifier le certificat SSL (à éviter en production).\"\"\"\n",
    "    context = ssl._create_unverified_context()\n",
    "    return urlopen(url, context=context)\n",
    "\n",
    "def alriyadh_scraper(alri):\n",
    "    contents_alri, raw_content = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(alri['URL']), desc=\"Processing Alriyadh URLs\"):\n",
    "        try:\n",
    "            response = open_url_no_verify(url)\n",
    "            html = response.read().decode('utf-8', errors='ignore')\n",
    "            raw_content.append(html)\n",
    "            \n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            title = soup.find('div', class_='article-title').text if soup.find('div', class_='article-title') else 'Titre non trouvé'\n",
    "            text = soup.find('div', class_='article-text').text if soup.find('div', class_='article-text') else 'Texte non trouvé'\n",
    "            \n",
    "            row_id = alri['Document ID'].iloc[i]\n",
    "\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,  # À ajuster si une image est présente sur la page\n",
    "                    'domain': None,  # Vous pouvez extraire le domaine si nécessaire\n",
    "                    'title': title,\n",
    "                    'cleaned_text': text,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Vous pouvez extraire la date de publication si nécessaire\n",
    "                }\n",
    "            }\n",
    "            contents_alri.append(article_data)  \n",
    "\n",
    "        except (HTTPError, URLError, ssl.SSLError, ValueError) as e:\n",
    "            print(f\"Erreur pour {url} : {e}\")\n",
    "            # Pour ne pas décaler les id, on rajoute quand même des valeurs par défaut (None)\n",
    "            row_id = alri['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents_alri.append(article_data)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return contents_alri, raw_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca413bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alri = data_false[data_false['Source Domain'] == 'alriyadh.com']\\ncontents_alri = alriyadh_scraper(alri[:5])\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"alri = data_false[data_false['Source Domain'] == 'alriyadh.com']\n",
    "contents_alri = alriyadh_scraper(alri[:5])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11659a",
   "metadata": {},
   "source": [
    "### sahafaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92d2fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pas possible en requests, pas d'appel api  ==> selenium\n",
    "# Ne marche pas avec goose .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a95fb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Liste des classes possibles à tester dans des <div> -> agregateur donc pas forcement tout le tps la meme structure\n",
    "    class_candidates = [\n",
    "        'text-body',\n",
    "        'story_text',\n",
    "        'post--content',\n",
    "        'post_details_inner',\n",
    "        \"news-content\",\n",
    "        \"news_text\",\n",
    "        \"article\",\n",
    "        \"description-text\",\n",
    "        'col-md-12',\n",
    "    ]\n",
    "\n",
    "    content_parts = []\n",
    "\n",
    "    for class_name in class_candidates:\n",
    "        if class_name == 'article':\n",
    "            found = soup.find_all('div', id=lambda c: c and 'article' in c)  # seul cas ou le texte est dans un div avec id (sinon class)\n",
    "        else:\n",
    "            found = soup.find_all('div', class_=lambda c: c and class_name in c)\n",
    "\n",
    "        if found and str(404) not in str(found):\n",
    "            for f in found:\n",
    "                text = f.get_text(strip=True)\n",
    "                if len(text) > 100:  # Ajout d'un contrôle sur la taille du texte\n",
    "                    content_parts.append(text)\n",
    "\n",
    "            if content_parts:\n",
    "                return {\n",
    "                    'content': \"##\".join(content_parts),\n",
    "                    'Title': soup.find('meta', property='og:title')['content'] if soup.find('meta', property='og:title') else \"Titre non trouvé\",\n",
    "                    'URL': url\n",
    "                }\n",
    "\n",
    "    print(f\"Aucun contenu trouvé dans les classes spécifiées pour l'url {url} ... \\n On cherche dans le og:description\")\n",
    "    meta = soup.find('meta', property='og:description')\n",
    "    if meta and meta.get('content') and '[…]' not in meta['content']:\n",
    "        return {\n",
    "            'content': meta['content'],\n",
    "            'title': soup.find('meta', property='og:title')['content'] if soup.find('meta', property='og:title') else \"Titre non trouvé\",\n",
    "            'url': url\n",
    "        }\n",
    "\n",
    "    print(f\"##### Aucun contenu trouvé pour l'url {url} ###### \\n\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def sahaafa_scraper(sahafaa):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--ignore-ssl-errors=yes')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    # options.add_argument('--headless')  \n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "\n",
    "    contents, raw_content = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(sahafaa['URL'].tolist()), desc=\"Processing sahaafa.net URLs\"):\n",
    "        row_id = sahafaa['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Essayer de passer dans un iframe si présent\n",
    "            try:\n",
    "                iframe = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"iframe\"))\n",
    "                )\n",
    "                driver.switch_to.frame(iframe)\n",
    "            except Exception as e:\n",
    "                contents.append(article_none)\n",
    "                print(f\"[{url}] Pas d'iframe trouvé ou erreur :\", e)\n",
    "\n",
    "            html = driver.page_source\n",
    "            raw_content.append(html)\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            elements = extract_article_text(html, url)\n",
    "\n",
    "            # Récupérer l'ID du document\n",
    "            row_id = sahafaa['Document ID'].iloc[i]\n",
    "\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': elements['URL']},\n",
    "                    'image': None,  # Vous pouvez extraire une image si nécessaire\n",
    "                    'domain': None,  # Vous pouvez extraire le domaine si nécessaire\n",
    "                    'title': elements[\"Title\"],\n",
    "                    'cleaned_text': elements[\"content\"],\n",
    "                    'opengraph': {},  # Vous pouvez compléter selon votre besoin\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Vous pouvez extraire la date de publication si nécessaire\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "            driver.switch_to.default_content()  # Revenir au contenu principal\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{url}] Erreur générale :\", e)\n",
    "            # Pour ne pas décaler les ID, on rajoute quand même des valeurs par défaut (None)\n",
    "            contents.append(article_none)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0baf8",
   "metadata": {},
   "source": [
    "### klyoum.com\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11709d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def klyoum_scraper(klyoum):\n",
    "    g = Goose()\n",
    "    contents, raw_content = [], []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "        'Referer': 'https://www.msn.com/',\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(klyoum['URL'].tolist()),desc='processing klyoum URLs'):\n",
    "        row_id = klyoum['Document ID'].iloc[i]\n",
    "        \n",
    "        # Article par défaut en cas d'erreur pour ne pas décaler les ids\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            contents.append(article_none)\n",
    "            raw_content.append(None)\n",
    "            continue\n",
    "        \n",
    "        raw_content.append(response.text)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        button = soup.find('a', attrs={\n",
    "            'onclick': \"gtag('event', 'click', {'event_category': 'link to original website','event_label': 'button_link to original website'});\"\n",
    "        })\n",
    "\n",
    "        if button and button.get('href'):\n",
    "            href = button.get('href')\n",
    "            params = parse_qs(urlparse(href).query)\n",
    "            domain = params.get('cc', [''])[0]\n",
    "            title = params.get('t', [''])[0]\n",
    "            \n",
    "            if domain and title:\n",
    "                params = {\n",
    "                    \"api_key\": \"77c5ebb6d414ddeb58b73c1eccddfcddedb5ce0996bb1a68f5bbd419513d8088\", #laura's one\n",
    "                    \"engine\": \"google\",\n",
    "                    \"hl\": \"fr\",\n",
    "                    \"q\": f'{str(domain)} {str(title)}'  # La vraie requête ici\n",
    "                }\n",
    "                search = GoogleSearch(params)\n",
    "                data = search.get_dict()\n",
    "\n",
    "                try:\n",
    "                    links = data['organic_results']\n",
    "                except KeyError as e:\n",
    "                    contents.append(article_none)\n",
    "                    print(f\"No organic value in the serpapi result for {url}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    if links:\n",
    "                        first_link = links[0][\"link\"]\n",
    "                        article = g.extract(first_link)\n",
    "                        print(\"daaate\", article.publish_date)\n",
    "                        article_data = {\n",
    "                            'row_id': row_id,\n",
    "                            'data': {\n",
    "                                'meta': {'meta_description': article.meta_description, 'url': first_link},\n",
    "                                'image': article.top_image.src if article.top_image else None,\n",
    "                                'domain': article.domain,\n",
    "                                'title': article.title,\n",
    "                                'cleaned_text': article.cleaned_text,\n",
    "                                'opengraph': article.opengraph,\n",
    "                                'tags': article.tags,\n",
    "                                'tweets': article.tweets,\n",
    "                                'movies': article.movies,\n",
    "                                'links': article.links,\n",
    "                                'authors': article.authors,\n",
    "                                'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                            }\n",
    "                        }\n",
    "                        contents.append(article_data)\n",
    "                    else:\n",
    "                        print(\"No Google result found for the URL.\")\n",
    "                        contents.append(article_none)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Google Search failed for {url}: {e}\")\n",
    "                    contents.append(article_none)\n",
    "            else:\n",
    "                print(f\"Missing parameters in href for {url}.\")\n",
    "                contents.append(article_none)\n",
    "        else:\n",
    "            print(f\"No button found for {url}.\")\n",
    "            contents.append(article_none)\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df989dca",
   "metadata": {},
   "source": [
    "### SPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "266af4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = data_false[data_false['Source Domain'] == 'spa.gov.sa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f65f784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spa_scraper(spa, timeout=150):\n",
    "    contents, raw_content = [], []\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "        'Referer': 'https://www.msn.com/',\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(spa['URL'].tolist()), desc=\"Processing spa URLs\"):\n",
    "        row_id = spa['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            # Effectuer la requête HTTP\n",
    "            response = requests.get(url, timeout=timeout, headers=headers)\n",
    "            raw_content.append(response.text)\n",
    "            \n",
    "            # Analyser la page avec BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Extraction du contenu de l'article avec Goose\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=response.text)\n",
    "            \n",
    "            title = article.title\n",
    "            meta = article.meta_description\n",
    "            \n",
    "            # Récupérer le contenu à partir de la balise og:description\n",
    "            content = soup.find(\"meta\", property=\"og:description\")['content'] if soup.find(\"meta\", property=\"og:description\") else \"\"\n",
    "            if content == \"\":\n",
    "                print(f\"Aucun contenu trouvé dans la balise og:description pour l'URL : {url}\")\n",
    "                continue  # Passer à l'URL suivante si aucun contenu n'est trouvé\n",
    "            \n",
    "            # Créer un dictionnaire de données à ajouter à `contents`\n",
    "            fields = [\n",
    "                'meta', 'image', 'domain', 'title', 'cleaned_text', 'opengraph',\n",
    "                'tags', 'tweets', 'movies', 'links', 'authors', 'publish_date'\n",
    "            ]\n",
    "            d = {field: \"\" for field in fields}\n",
    "            d['title'] = title\n",
    "            d['cleaned_text'] = content\n",
    "            d['meta'] = meta\n",
    "            \n",
    "            # Récupérer l'ID du document\n",
    "            row_id = spa['Document ID'].iloc[i]\n",
    "            \n",
    "            # Ajouter les données de l'article à la liste `contents`\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': d\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "            print(f\"Contenu extrait pour {url}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nErreur de requête pour {url}: {e}\")\n",
    "            # Pour ne pas décaler les ID, on rajoute quand même des valeurs par défaut (None)\n",
    "            \n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Gestion d'autres erreurs\n",
    "            print(f\"Erreur d'extraction pour {url}: {e}\")\n",
    "            \n",
    "            contents.append(article_none)\n",
    "\n",
    "        # Ajouter un délai pour éviter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ddc19",
   "metadata": {},
   "source": [
    "### aawsat.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6f434a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aawsat_scraper(aawsat):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "    contents, raw_contents = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(aawsat['URL']), desc=\"Processing aawsat URLs\"):\n",
    "        row_id = aawsat['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            raw_contents.append(response.text)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                g = Goose()\n",
    "                article = g.extract(raw_html=response.text)\n",
    "                row_id = aawsat['Document ID'].iloc[i]\n",
    "                \n",
    "                article_data = {\n",
    "                    'row_id': row_id,\n",
    "                    'data': {\n",
    "                        'meta': {'meta_description': article.meta_description, 'url': url},\n",
    "                        'image': article.top_image.src if article.top_image else None,\n",
    "                        'domain': article.domain,\n",
    "                        'title': article.title,\n",
    "                        'cleaned_text': article.cleaned_text,\n",
    "                        'opengraph': article.opengraph,\n",
    "                        'tags': article.tags,\n",
    "                        'tweets': article.tweets,\n",
    "                        'movies': article.movies,\n",
    "                        'links': article.links,\n",
    "                        'authors': article.authors,\n",
    "                        'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                    }\n",
    "                }\n",
    "                contents.append(article_data)\n",
    "\n",
    "            else:\n",
    "                print(f\"Accès échoué à : {url} avec le code {response.status_code}\")\n",
    "                # Pour ne pas décaler les ID, on rajoute quand même None\n",
    "                \n",
    "                contents.append(article_none)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement de {url}: {e}\")\n",
    "            \n",
    "            contents.append(article_none)\n",
    "\n",
    "        # Ajout d'un délai pour éviter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return contents, raw_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f40ff5",
   "metadata": {},
   "source": [
    "### RT.com & NYT & CNN & Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60f804e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = data_false[data_false['Source Domain'] == 'rt.com']\n",
    "nytimes = data_false[data_false['Source Domain'] == 'nytimes.com']\n",
    "cnn = data_false[data_false['Source Domain'] == 'cnn.com']\n",
    "yahoo = data_false[data_false['Source Domain'] == 'yahoo.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "986bd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_nyt_cnn_scraper(rt):\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    contents,raw_contents = [],[]\n",
    "    for i,url in tqdm(enumerate(rt['URL'].tolist()), desc=\"Processing rt/nyt/cnn URLs\"):\n",
    "        row_id = rt['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id' : row_id,\n",
    "            'data' : {\n",
    "            'meta': {'canonical': url},\n",
    "            'image': None,\n",
    "            'domain': None,\n",
    "            'title': None,\n",
    "            'cleaned_text': None,\n",
    "            'opengraph': {},\n",
    "            'tags': [],\n",
    "            'tweets': [],\n",
    "            'movies': [],\n",
    "            'links': [],\n",
    "            'authors': [],\n",
    "            'publish_date': None\n",
    "        }}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers= headers)\n",
    "            response.raise_for_status()  \n",
    "            raw_contents.append(response.text)\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=response.text)\n",
    "            row_id = rt['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id' : row_id,\n",
    "                'data' : {\n",
    "                    'meta': article.meta_description,\n",
    "                    'image': article.top_image.src if article.top_image else None,\n",
    "                    'domain': article.domain,\n",
    "                    'title': article.title,\n",
    "                    'cleaned_text': article.cleaned_text,\n",
    "                    'opengraph': article.opengraph,\n",
    "                    'tags': article.tags,\n",
    "                    'tweets': article.tweets,\n",
    "                    'movies': article.movies,\n",
    "                    'links': article.links,\n",
    "                    'authors': article.authors,\n",
    "                    'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nErreur de requête pour {url} : {e}\")\n",
    "            #pour ne pas décaler les id on rajotue quand même None\n",
    "            \n",
    "            contents.append(article_none)\n",
    "        except Exception as e:\n",
    "            #pour ne pas décaler les id on rajotue quand même None\n",
    "            contents.append(article_none)\n",
    "            print(f\"\\nErreur d'extraction pour {url} : {e}\")\n",
    "\n",
    "        time.sleep(0.5) \n",
    "\n",
    "    return contents,raw_contents\n",
    "\n",
    "\n",
    "#content = rt_nyt_cnn_scraper(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694746c",
   "metadata": {},
   "source": [
    "### Investing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c11e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "investing = data_false[data_false['Source Domain'] == 'investing.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5007781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investing_scraper(invest):\n",
    "    contents, raw_content = [], []\n",
    "    \n",
    "    # Initialisation des options de Chrome\n",
    "    options = webdriver.ChromeOptions()  # Pas de headless sinon première attente ne fonctionne pas\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--ignore-ssl-errors=yes')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "\n",
    "    for i,url in tqdm(enumerate(invest['URL'].tolist())):\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        try:\n",
    "            driver.get(url)\n",
    "\n",
    "            # Attendre que le titre de la page soit chargé, il y a un délai de 3s pour charger la page \n",
    "            WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.TAG_NAME, \"title\")))\n",
    "\n",
    "            raw_content.append(driver.page_source)\n",
    "            element = driver.find_element(By.TAG_NAME, 'h1')\n",
    "\n",
    "            html = driver.page_source\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=html)\n",
    "            row_id = invest['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data':{\n",
    "                'meta': article.meta_description,\n",
    "                'image': article.top_image.src if article.top_image else None,\n",
    "                'domain': article.domain,\n",
    "                'title': article.title,\n",
    "                'cleaned_text': article.cleaned_text,\n",
    "                'opengraph': article.opengraph,\n",
    "                'tags': article.tags,\n",
    "                'tweets': article.tweets,\n",
    "                'movies': article.movies,\n",
    "                'links': article.links,\n",
    "                'authors': article.authors,\n",
    "                'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "            }}\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur est survenue pour l'URL {url}: {e}\")\n",
    "            #pour ne pas décaler les id on rajotue quand même None\n",
    "            row_id = invest['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id' : row_id,\n",
    "                'data' : {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }}\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        finally:\n",
    "            driver.quit() #obliger de le fermer à chaque it \n",
    "\n",
    "    return contents, raw_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e2d5b",
   "metadata": {},
   "source": [
    "### Sohu.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b15b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sohu = data_false[data_false['Source Domain'] == 'sohu.com']\n",
    "#goose ne marche pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c641d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sohu_scraper(sohu):\n",
    "    contents, raw_contents = [], []\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    appended = False\n",
    "    for i, url in tqdm(enumerate(sohu['URL'].tolist()), desc=\"Processing sohu URLs\"):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            raw_contents.append(response.text)\n",
    "            appended = True\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Récupération du titre\n",
    "            title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
    "\n",
    "            # Extraction du contenu de l'article\n",
    "            content_div = soup.find('div', attrs={'data-spm': 'content'})\n",
    "            content = content_div.get_text(strip=True, separator=\"\\n\").strip() if content_div else \"\"\n",
    "\n",
    "            row_id = sohu['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': title,\n",
    "                    'cleaned_text': content,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur de requête pour {url} : {e}\")\n",
    "            # Pour ne pas décaler les ID, on ajoute quand même un article avec des valeurs par défaut (None)\n",
    "            row_id = sohu['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            if not appended : raw_contents.append(None)\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur d'extraction pour {url} : {e}\")\n",
    "            # Pour ne pas décaler les ID, on ajoute quand même un article avec des valeurs par défaut (None)\n",
    "            row_id = sohu['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            if not appended : raw_contents.append(None)\n",
    "            contents.append(article_data)\n",
    "\n",
    "        # Ajout d'un délai pour éviter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return contents, raw_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42cf19",
   "metadata": {},
   "source": [
    "### Goal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3648512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_scraper(goal):\n",
    "    contents, raw_content = [], []\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(goal['URL'].tolist()), desc=\"Processing goal.com URLs\"):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            raw_content.append(response.text)\n",
    "\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=response.text)\n",
    "\n",
    "            row_id = goal['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': article.meta_description,\n",
    "                    'image': article.top_image.src if article.top_image else None,\n",
    "                    'domain': article.domain,\n",
    "                    'title': article.title,\n",
    "                    'cleaned_text': article.cleaned_text,\n",
    "                    'opengraph': article.opengraph,\n",
    "                    'tags': article.tags,\n",
    "                    'tweets': article.tweets,\n",
    "                    'movies': article.movies,\n",
    "                    'links': article.links,\n",
    "                    'authors': article.authors,\n",
    "                    'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nErreur de requête pour {url} : {e}\")\n",
    "            # Pour ne pas décaler les ID, on ajoute un article avec des valeurs par défaut (None)\n",
    "            row_id = goal['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Gestion des autres erreurs\n",
    "            print(f\"\\nErreur d'extraction pour {url} : {e}\")\n",
    "            # Même traitement que pour les erreurs de requête\n",
    "            row_id = goal['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "\n",
    "        time.sleep(0.5)  # Délai pour éviter de surcharger le serveur\n",
    "\n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1624d",
   "metadata": {},
   "source": [
    "## Gather all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "270ea45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lecture du df meltwater, pour chaque url soit on scrapp normal (selenium) soit on scrapped avec la fonction ad&quate en fonction de la source \n",
    "# Scrapping puis enregistrement dans /content (pour le fichier source) et /clean_content (pour le json output)\n",
    "# Chaque algo de scrap manuel renvoie une liste de json correspnod à chacun de ses articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e454d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self, data, manualy_scrapped_sources):\n",
    "        self.data = data\n",
    "        self.manualy_scrapped_sources = manualy_scrapped_sources\n",
    "        self.source_domains = data['Source Domain'].unique()\n",
    "        self.minister = None\n",
    "        \n",
    "\n",
    "        sys.stdout = open('output.log', 'w')\n",
    "        print(\"Everything from now on goes to the log file.\")\n",
    "\n",
    "    def route_scrap(self):\n",
    "        # Supprimer les dossiers 'content' et 'cleaned_content' s'ils existent\n",
    "        content_folder = 'content'\n",
    "        cleaned_content_folder = 'cleaned_content'\n",
    "        \n",
    "        if os.path.isdir(content_folder):\n",
    "            shutil.rmtree(content_folder)  # Supprimer le dossier 'content'\n",
    "            print(f\"Le dossier {content_folder} a été supprimé.\")\n",
    "        \n",
    "        if os.path.isdir(cleaned_content_folder):\n",
    "            shutil.rmtree(cleaned_content_folder)  # Supprimer le dossier 'cleaned_content'\n",
    "            print(f\"Le dossier {cleaned_content_folder} a été supprimé.\")\n",
    "        \n",
    "        # Traiter chaque source\n",
    "        for source in self.source_domains:\n",
    "            source_data = self.data[self.data['Source Domain'] == source]\n",
    "\n",
    "            if source in self.manualy_scrapped_sources:\n",
    "                self.manual_scrap(source_data)\n",
    "            else:\n",
    "                self.default_scrap(source_data)\n",
    "\n",
    "            # Libérer la mémoire en supprimant les données déjà traitées\n",
    "            self.data = self.data[self.data['Source Domain'] != source].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \n",
    "    def check_condition(self, article):\n",
    "        if list(article.keys()) == ['row_id','data'] : \n",
    "            article = article['data']\n",
    "        try : \n",
    "            if not article : \n",
    "                print(\"not article\")\n",
    "            if len(article['cleaned_text'])<50:\n",
    "                return False\n",
    "            #if 'I been blocked' in str(article[\"infos\"]):\n",
    "            #    return False\n",
    "            if 'changement de réseau' in str(article['cleaned_text']):\n",
    "                return False\n",
    "            if 'cloudflare' in str(article['title']):\n",
    "                return False\n",
    "            #if 'cloudflare' in str(article.infos):\n",
    "            #    return False\n",
    "            if 'cloudflare' in str(article['cleaned_text']):\n",
    "                return False\n",
    "            if '404 Error' in str(article['title']):\n",
    "                return False\n",
    "            return True\n",
    "        except : \n",
    "            print(\"Article non valide pour le check conditions\")\n",
    "            return False\n",
    "            \n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "    ################# Use of manual scraping, coded above ##########################\n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    def manual_scrap(self, source_data):\n",
    "        \"\"\"\n",
    "        Nos scrapers manuels renvoient deux objets : content (qui est une liste de clean json) et raw content (une liste de fichiers .txt de page source)\n",
    "        A partir de cela on peut save les articles dans content et /clean_content avec l'id row fourni dans le \"content\" \n",
    "        \n",
    "        ####\n",
    "        content.keys() = [row_id, data] où row_id est l'id du document et data est le json d'intérêt \n",
    "        type(raw_content) = .txt\n",
    "        \"\"\"\n",
    "        source_name = source_data['Source Domain'].iloc[0]\n",
    "        print(f\"Manual scraping for {source_name}\")\n",
    "        if source_name == 'msn.com' :\n",
    "            content, raw_content = msn_scraper(source_data)\n",
    "        elif source_name == 'sauress.com' :\n",
    "            content,raw_content = sauress_scraper(source_data)\n",
    "        elif source_name == 'aawsat.com' :\n",
    "            content,raw_content = aawsat_scraper(source_data)\n",
    "        elif source_name == 'alriyadh.com' :\n",
    "            content,raw_content = alriyadh_scraper(source_data)\n",
    "        elif source_name == 'klyoum.com' :\n",
    "            content,raw_content = klyoum_scraper(source_data)\n",
    "        elif source_name == 'spa.gov.sa' :\n",
    "            content,raw_content = spa_scraper(source_data)\n",
    "        elif source_name == 'rt.com' or source_name == 'nytimes.com' or source_name == 'cnn.com' or source_name == 'yahoo.com' :\n",
    "            content,raw_content = rt_nyt_cnn_scraper(source_data)\n",
    "        elif source_name == 'shafaqna.com' :\n",
    "            content,raw_content = shafaqna_scraper(source_data)\n",
    "        elif source_name == 'goal.com' :\n",
    "            content,raw_content = goal_scraper(source_data)\n",
    "        elif source_name == 'sohu.com' :\n",
    "            content,raw_content = sohu_scraper(source_data)\n",
    "        elif source_name == 'investing.com' :\n",
    "            content,raw_content = investing_scraper(source_data) \n",
    "        else : \n",
    "            raise ValueError(\"Source Domain non valide pour le manual scrapping\")\n",
    "        \n",
    "        self.minister = source_data['RB Minister'].iloc[0] #on suppose que le minister est le même dans tout le batch de données \n",
    "        # Sauvegarder le contenu brut et nettoyé\n",
    "        self.manual_save_content(raw_content = raw_content, content = content)\n",
    "        self.manual_save_clean_content(cleaned_content = content)\n",
    "    \n",
    "    def manual_save_content(self, raw_content, content):\n",
    "        \"\"\"\n",
    "        Sauvegarder le contenu brut pour une source spécifique\n",
    "        `raw_content` = liste de fichiers .txt \n",
    "        `content` = liste de json où chaque article contient une clé 'row_id'\n",
    "        \"\"\"\n",
    "        \n",
    "        folder_path =f\"content/{self.minister}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)  \n",
    "        for i, article in enumerate(raw_content):  # Sauvegarder chaque fichier .txt de raw_content\n",
    "            row_id = content[i]['row_id']  # Utiliser 'row_id' du contenu\n",
    "            file_name = f\"{row_id}.txt\"  # Nom du fichier basé sur 'row_id'\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(article)\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def manual_save_clean_content(self, cleaned_content):\n",
    "        \"\"\"\n",
    "        Sauvegarder le contenu nettoyé pour une source spécifique au format JSON, fait automatiquement normalement par les algo de scrap manuels\n",
    "        \"\"\"\n",
    "        folder_path = f\"cleaned_content/{self.minister}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)  \n",
    "\n",
    "        for i, article in enumerate(cleaned_content):  \n",
    "            if not self.check_condition(article=article) :  #possible que le clean json soit vide (champs vides)\n",
    "                print(f\"Article {article} does not meet the cleaning criteria.\")\n",
    "                continue\n",
    "            row_id = article['row_id'] \n",
    "            file_name = f\"{row_id}_cleaned.json\" \n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            cleaned_data = article['data']#on ne veux pas row_id \n",
    "\n",
    "            try:\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "                    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=4)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving cleaned content for {file_name}: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "    ####### Default Scrapping with firas's method with selenium and goose ##########\n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "    \n",
    "\n",
    "    def default_save_content(self, source_data, max_retries=2):\n",
    "        \"\"\"Appel pour chaque source, et renvoie le folder dans lequel il a enregistrer les contents (utile dans le cas où l'on a plusieurs ministre)\n",
    "        et le nombre d'articles appended, pour ensuite clean seulement ceux qui nous interesse et pas parcourir tout le folder /content\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--ignore-ssl-errors=yes')\n",
    "        options.add_argument('--ignore-certificate-errors')\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(30)  # Timeout pour le chargement de la page\n",
    "\n",
    "        article_saved = 0\n",
    "        folder_path = 'Error'\n",
    "        if len(source_data) == 0:\n",
    "            print(\"Source data is empty. Exiting.\")\n",
    "            driver.quit()\n",
    "            return folder_path, article_saved\n",
    "        for index, row in tqdm(source_data.iterrows(), total=source_data.shape[0]):\n",
    "            \n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "\n",
    "\n",
    "                try:\n",
    "                    driver.get(row[\"URL\"])\n",
    "\n",
    "                    time.sleep(3)  # Attendre quelques secondes pour que la page se charge\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                    self.minister = row['RB Minister']\n",
    "                    folder_path = f\"content/{self.minister}\"\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                    # Sauvegarder le contenu de la page dans un fichier texte\n",
    "                    with open(f\"{folder_path}/{row['Document ID']}.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "                        text_file.write(str(soup))\n",
    "\n",
    "                    article_saved +=1\n",
    "                    break  # Si tout s'est bien passé, sortir de la boucle de réessai\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {row['URL']}: {e}\")\n",
    "                    retries += 1\n",
    "                    print(f\"Tentative {retries} sur {max_retries} pour {row['URL']}\")\n",
    "                    time.sleep(3) \n",
    "\n",
    "                    if retries == max_retries:\n",
    "                        print(f\"Échec après {max_retries} tentatives pour {row['URL']}\")\n",
    "                        folder_path = \"Error\"\n",
    "                        break  # Passer à l'URL suivante après le nombre maximum de tentatives\n",
    "                        \n",
    "\n",
    "        driver.quit()\n",
    "        return folder_path, article_saved\n",
    "    \n",
    "\n",
    "    def default_save_clean_content(self, content_folder_path, n):\n",
    "        \"\"\"On récupere les n derniers fichiers saved par la fonction save_content puis on vérifie si ils peuvent être cleané.\n",
    "        Si oui on le save, sinon on le passe. \"\"\"\n",
    "        \n",
    "        if not os.path.isdir(content_folder_path):\n",
    "            print(f'Error: {content_folder_path} does not exist or is not a directory.')\n",
    "            return  #on a aucun article à clean puisque le content_folder == Error ou n'existe pas \n",
    "\n",
    "        g = Goose()\n",
    "        results = []\n",
    "\n",
    "        for filename in tqdm(os.listdir(content_folder_path)[-n:]):  # On s'intéresse seulement aux n derniers articles sauvegardés\n",
    "            \n",
    "            file_path = os.path.join(content_folder_path, filename)\n",
    "\n",
    "            if not os.path.isfile(file_path):\n",
    "                print(f'le file path : {file_path} n\\'est pas un file path valide')\n",
    "                continue  \n",
    "                \n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    html_raw = file.read()\n",
    "                article = g.extract(raw_html=html_raw)\n",
    "                if type(article.publish_date) == list:\n",
    "                    article.publish_date = article.publish_date[0]\n",
    "                    print(\"conversion date de liste à date\")\n",
    "                cleaned_data = {\n",
    "                    'meta': article.meta_description,\n",
    "                    'image': article.top_image.src if article.top_image else None,\n",
    "                    'domain': article.domain,\n",
    "                    'title': article.title,\n",
    "                    'cleaned_text': article.cleaned_text,\n",
    "                    'opengraph': article.opengraph,\n",
    "                    'tags': article.tags,\n",
    "                    'tweets': article.tweets,\n",
    "                    'movies': article.movies,\n",
    "                    'links': article.links,\n",
    "                    'authors': article.authors,\n",
    "                    'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                }\n",
    "\n",
    "                if not self.check_condition(article=cleaned_data): \n",
    "                    print(f'le fichier {cleaned_data}.txt ne répond pas aux critères de cleaning ...... ')\n",
    "                    continue  \n",
    "\n",
    "                \n",
    "\n",
    "                document_id = os.path.splitext(filename)[0]  # Extraire l'id avant le .txt\n",
    "                \n",
    "                output_folder = f\"cleaned_content/{self.minister}\"\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                output_file = os.path.join(output_folder, f\"{document_id}_clean.json\")\n",
    "\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "                    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")  # Afficher l'erreur si une exception se produit pendant le traitement\n",
    "                continue\n",
    "\n",
    "        print(\"Cleaning process completed.\")\n",
    "\n",
    "\n",
    "    def default_scrap(self, source_data):\n",
    "        '''Main fonction pour scrapper des sources de base : on append tous les articles.txt de la source dans /content puis on s'occupe de les cleans'''\n",
    "        content_folder_path,n = self.default_save_content(source_data)\n",
    "        self.default_save_clean_content(content_folder_path,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c195954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"20250428_Reema_Bandar_Al_Saud_Meltwater raw data (2024).xlsx\")\n",
    "scrapped_article = pd.read_excel(\"12-05-2025-SourceDomainReport.xlsx\")\n",
    "manualy_scraped_sources = scrapped_article[scrapped_article['Scrapped'] == True]['Source Domain'].tolist()\n",
    "unscrappable = scrapped_article[scrapped_article['Scrapped'] == False]['Source Domain'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fa6841ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RB Minister'] = [\"Reema_Bandar\"] * len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data[\"Source Domain\"].isin(unscrappable)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b7efccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "100%|██████████| 6/6 [00:22<00:00,  3.76s/it]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.07it/s]\n",
      "Processing Alriyadh URLs: 106it [01:15,  1.41it/s]\n",
      "processing klyoum URLs: 60it [02:13,  2.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m scraper \u001b[38;5;241m=\u001b[39m Scraper(data \u001b[38;5;241m=\u001b[39mdata,manualy_scrapped_sources\u001b[38;5;241m=\u001b[39mmanualy_scraped_sources)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute_scrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[140], line 30\u001b[0m, in \u001b[0;36mScraper.route_scrap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m source_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource Domain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m source]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanualy_scrapped_sources:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_scrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_scrap(source_data)\n",
      "Cell \u001b[0;32mIn[140], line 92\u001b[0m, in \u001b[0;36mScraper.manual_scrap\u001b[0;34m(self, source_data)\u001b[0m\n\u001b[1;32m     90\u001b[0m     content,raw_content \u001b[38;5;241m=\u001b[39m alriyadh_scraper(source_data)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mklyoum.com\u001b[39m\u001b[38;5;124m'\u001b[39m :\n\u001b[0;32m---> 92\u001b[0m     content,raw_content \u001b[38;5;241m=\u001b[39m \u001b[43mklyoum_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspa.gov.sa\u001b[39m\u001b[38;5;124m'\u001b[39m :\n\u001b[1;32m     94\u001b[0m     content,raw_content \u001b[38;5;241m=\u001b[39m spa_scraper(source_data)\n",
      "Cell \u001b[0;32mIn[144], line 60\u001b[0m, in \u001b[0;36mklyoum_scraper\u001b[0;34m(klyoum)\u001b[0m\n\u001b[1;32m     53\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m77c5ebb6d414ddeb58b73c1eccddfcddedb5ce0996bb1a68f5bbd419513d8088\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#laura's one\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(domain)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(title)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# La vraie requête ici\u001b[39;00m\n\u001b[1;32m     58\u001b[0m }\n\u001b[1;32m     59\u001b[0m search \u001b[38;5;241m=\u001b[39m GoogleSearch(params)\n\u001b[0;32m---> 60\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     links \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morganic_results\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:103\u001b[0m, in \u001b[0;36mSerpApiClient.get_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m        Dict with the formatted response content\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m        (alias for get_dictionary)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:96\u001b[0m, in \u001b[0;36mSerpApiClient.get_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dictionary\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        Dict with the formatted response content\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:83\u001b[0m, in \u001b[0;36mSerpApiClient.get_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Formatted JSON search results using json package\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:70\u001b[0m, in \u001b[0;36mSerpApiClient.get_results\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/search\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m        Response text field\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:59\u001b[0m, in \u001b[0;36mSerpApiClient.get_response\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     57\u001b[0m     url, parameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_url(path)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# print(url)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scraper = Scraper(data =data,manualy_scrapped_sources=manualy_scraped_sources)\n",
    "scraper.route_scrap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envFormationP2)",
   "language": "python",
   "name": "envformationp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
