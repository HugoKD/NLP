{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fde5208",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "988ecfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "from serpapi import GoogleSearch as search\n",
    "from tqdm import tqdm\n",
    "from timeout_decorator import timeout\n",
    "import json\n",
    "import shutil\n",
    "import os \n",
    "import pandas as pd\n",
    "from goose3 import Goose\n",
    "import ssl\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from serpapi import GoogleSearch\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "651507ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lacunaire = pd.read_excel('/Users/hugo.cadet/PycharmProjects/Meltwater_tag.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1778719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/Users/hugo.cadet/Downloads/Copie de 20241202_Mohammed_bin_Salman_Meltwater data excluding Twitter.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c522fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(data_lacunaire[['Document ID','Scrapped']], on='Document ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc179d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ini = data.copy()\n",
    "data = data.dropna(subset=['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9ea1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_false = data[data['Scrapped'] == False]\n",
    "reach_by_domain = data_false.groupby('Source Domain')['Reach'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48cbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true = data[data['Scrapped'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e036759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (data_false.groupby('Source Domain').size() / len(data_false)).reset_index()\n",
    "p1.columns = ['Source Domain', 'Share of Unscraped Articles per Source Among All Unscraped Articles']\n",
    "\n",
    "p1['Number of Unscraped Articles'] = data_false['Source Domain'].value_counts().reindex(p1['Source Domain']).values\n",
    "\n",
    "reach_per_source = (\n",
    "    data_false.groupby('Source Domain')['Reach']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'Reach': 'reach'})\n",
    ")\n",
    "p1 = p1.merge(reach_per_source, on='Source Domain', how='left')\n",
    "\n",
    "false_counts = data_false['Source Domain'].value_counts()\n",
    "total_counts = data['Source Domain'].value_counts()\n",
    "proportion_total = (false_counts / total_counts).reset_index()\n",
    "proportion_total.columns = ['Source Domain', 'Unscraped Articles Share per Source']\n",
    "\n",
    "excel_final = p1.merge(proportion_total, on='Source Domain', how='left')\n",
    "\n",
    "excel_final = excel_final[\n",
    "    [\n",
    "        'Source Domain',\n",
    "        'reach',\n",
    "        'Number of Unscraped Articles',\n",
    "        'Share of Unscraped Articles per Source Among All Unscraped Articles',\n",
    "        'Unscraped Articles Share per Source'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca04fb",
   "metadata": {},
   "source": [
    "### msn.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd1c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msn = data_false[data_false['Source Domain'] == 'msn.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1fedff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Final Version ####\n",
    "## goose non possible\n",
    "# Oblig√© de r√©cup l'id dans l'url puis de l'utiliser dans l'API\n",
    "\n",
    "def get_ids(msn_urls): \n",
    "    ids = []\n",
    "    for url in msn_urls:\n",
    "        url_parts = str(url).split(\"-\")  \n",
    "        last_part = url_parts[-1]\n",
    "        if len(last_part) == 8:\n",
    "            ids.append(last_part)\n",
    "        elif len(last_part) == 17:\n",
    "            ids.append(last_part[:-9])  # Enl√®ve le suffixe \"#comment\"\n",
    "        elif len(last_part) == 5 and len(url_parts) >= 3:\n",
    "            ids.append(url_parts[-3][:8])\n",
    "        else:\n",
    "            ids.append(None)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def msn_scraper(msn, is_url=False, urls=None):\n",
    "    contents = []\n",
    "    raw_content = []\n",
    "\n",
    "    if is_url:  \n",
    "        urls = urls\n",
    "    else: \n",
    "        urls = msn['URL'].tolist()\n",
    "\n",
    "    ids = get_ids(urls)\n",
    "\n",
    "    for i, id_ in tqdm(enumerate(ids), desc='Processing msn URLs'):\n",
    "        if id_ is None:\n",
    "            print(f\"ID manquant pour URL : {urls[i]}\")\n",
    "            row_id = msn['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                \"row_id\": row_id,      \n",
    "                'data': {\n",
    "                    'meta': {'canonical': urls[i]},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "            raw_content.append(None)\n",
    "            continue\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "            'Referer': 'https://www.msn.com/',\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Faire une requ√™te pour r√©cup√©rer les donn√©es de l'API\n",
    "            response = requests.get(\n",
    "                f'https://assets.msn.com/content/view/v2/Detail/en-us/{id_}', \n",
    "                headers=headers, timeout=10\n",
    "            )\n",
    "            raw_content.append(response.text)\n",
    "            response.raise_for_status()  # V√©rifie si la requ√™te a r√©ussi (status code 200)\n",
    "\n",
    "            # Extraire les donn√©es JSON et le contenu HTML\n",
    "            data = response.json()\n",
    "            title = data.get('title', '')\n",
    "            body_html = data.get('body', '')\n",
    "            \n",
    "            # Utilisation de BeautifulSoup pour extraire le texte\n",
    "            soup = BeautifulSoup(body_html, \"html.parser\")\n",
    "            content_text = soup.get_text(separator=\"\\n\")  # R√©cup√©rer le texte\n",
    "\n",
    "            # Ajouter les donn√©es extraites dans le r√©sultat\n",
    "            row_id = msn['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                \"row_id\": row_id,      \n",
    "                'data': {\n",
    "                    'meta': {'canonical': urls[i]},\n",
    "                    'image': None,  # Vous pouvez ajouter une logique pour r√©cup√©rer une image si n√©cessaire\n",
    "                    'domain': None,  # Vous pouvez extraire le domaine si n√©cessaire\n",
    "                    'title': title,\n",
    "                    'cleaned_text': content_text,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Vous pouvez extraire la date de publication si n√©cessaire\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur pour l'ID {id_} : {e}\")\n",
    "            # En cas d'erreur, ajouter un article avec des valeurs par d√©faut\n",
    "            row_id = msn['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                \"row_id\": row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': urls[i]},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "\n",
    "        # Ajout d'un d√©lai pour √©viter de surcharger le serveur\n",
    "        time.sleep(0.5)  \n",
    "\n",
    "    return contents, raw_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2445f31",
   "metadata": {},
   "source": [
    "### Sauress.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "025693c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sauress = data_false[data_false['Source Domain'] == 'sauress.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b5a7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sauress_scraper(sauress):\n",
    "    contents_sauress, raw_content = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(sauress['URL']), total=len(sauress['URL']), desc=\"Processing sauress URLs\"):\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "                'Referer': 'https://www.msn.com/',\n",
    "            }\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            raw_content.append(response.text)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                # V√©rification du titre de la page\n",
    "                title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
    "                if not title:\n",
    "                    og_title = soup.find(\"meta\", property=\"og:title\")\n",
    "                    if og_title and og_title.get(\"content\"):\n",
    "                        title = og_title[\"content\"].strip()  \n",
    "            else:\n",
    "                print(\"Page non disponible ou code HTTP diff√©rent de 200\")\n",
    "                title = \"Titre non disponible\"\n",
    "            \n",
    "            # Extraction du texte principal\n",
    "            spans = soup.find_all(\"span\", class_=\"articlecontent\")\n",
    "            row_id = sauress['Document ID'].iloc[i]\n",
    "            \n",
    "            # Construction des donn√©es de l'article\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,  # Ajouter la logique pour r√©cup√©rer l'image si n√©cessaire\n",
    "                    'domain': None,  # Ajouter la logique pour r√©cup√©rer le domaine si n√©cessaire\n",
    "                    'title': title,\n",
    "                    'cleaned_text': ', '.join([span.text.strip() for span in spans]),\n",
    "                    'opengraph': {},  # Ajouter les donn√©es OpenGraph si n√©cessaire\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Ajouter la logique pour r√©cup√©rer la date de publication si n√©cessaire\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents_sauress.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nüîó {url}\")\n",
    "            print(f\"Erreur de requ√™te : {e}\")\n",
    "            # Ajouter des donn√©es par d√©faut en cas d'erreur\n",
    "            row_id = sauress['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents_sauress.append(article_data)\n",
    "\n",
    "    return contents_sauress, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb47e37",
   "metadata": {},
   "source": [
    "### shafaqna.com\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "451af5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, scraper l'article avant la div latest news, si il n'y a pas d'article avant cela veut dire qu'il n'est plus dispo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a6470e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://egypt.shafaqna.com/AR/AL/',\n",
       "       'https://qatar.shafaqna.com/AR/AL/',\n",
       "       'http://malaysia.shafaqna.com/MY/AL/',\n",
       "       'https://algeria.shafaqna.com/', 'https://uae.shafaqna.com/AR/AL/',\n",
       "       'https://indonesia.shafaqna.com/ID/AL/',\n",
       "       'http://kenya.shafaqna.com/EN/A',\n",
       "       'https://thailand.shafaqna.com/TH/AL/',\n",
       "       'http://zimbabwe.shafaqna.com/EN/AL/',\n",
       "       'https://venezuela.shafaqna.com/ES/AL/',\n",
       "       'http://bahrain.shafaqna.com/AR/AL',\n",
       "       'http://poland.shafaqna.com/PL/AL/',\n",
       "       'http://taiwan.shafaqna.com/CN/AL/',\n",
       "       'http://chile.shafaqna.com/ES/AL/',\n",
       "       'https://www.pk.shafaqna.com/EN/AL/',\n",
       "       'https://singapore.shafaqna.com/EN/AL/',\n",
       "       'https://fa.shafaqna.com/news/1731394/%d8%b9%d8%b1%d8%a8%d8%b3%d8%aa%d8%a7%d9%86-%d8%a8%db%8c%d8%b4-%d8%a7%d8%b2-%db%b8%db%b5-%d8%af%d8%b1%d8%b5%d8%af-%d8%a7%d8%b2-%d8%aa%d9%88%d8%b5%db%8c%d9%87-%d9%87%d8%a7%db%8c-%d8%b3',\n",
       "       'https://algeria.shafaqna.com/AR/AL/',\n",
       "       'http://malaysia.shafaqna.com/EN/AL/'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shafaqna = data_false[data_false['Source Domain'] == 'shafaqna.com']\n",
    "shafaqna['URL'].apply(lambda x : x[:-7]).unique()\n",
    "#shafaqna['URL'].apply(lambda x : x[-7:]).tolist()\n",
    "# Meme format : 'https://egypt.shafaqna.com/../ + int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc2e2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shafaqna_scraper(shafaqna):\n",
    "    content, raw_content = [], []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "        'Referer': 'https://www.msn.com/',\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(shafaqna['URL']), desc='Processing shafaqna URLs'):\n",
    "        row_id = shafaqna['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, verify=False, headers=headers)\n",
    "            raw_content.append(response.text)\n",
    "\n",
    "            # V√©rification du code de statut\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Page non disponible ou code HTTP diff√©rent de 200 : {url}\")\n",
    "                \n",
    "                content.append(article_none)\n",
    "                continue\n",
    "\n",
    "            # Extraire le contenu principal de la page\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            news = soup.find(\"div\", id=\"right-column\")\n",
    "            news = news.find(\"div\", id=\"news-text\") if news else None\n",
    "\n",
    "            if not news:\n",
    "                print(f\"No news found in the url number {i} : {url}\")\n",
    "                \n",
    "                content.append(article_none)\n",
    "                continue\n",
    "\n",
    "            first_non_newsbox_div = news.find(\n",
    "                lambda tag: (\n",
    "                    tag.name == \"div\"\n",
    "                    and tag.get(\"class\") != [\"news\", \"box\"]\n",
    "                    and tag.get(\"id\") != \"titlebar\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if first_non_newsbox_div:\n",
    "                href = first_non_newsbox_div.find('a')['href']\n",
    "\n",
    "                # Faire une requ√™te pour l'article li√©\n",
    "                article_html = requests.get(href, timeout=10).text\n",
    "\n",
    "                # Extraire les donn√©es de l'article avec Goose\n",
    "                g = Goose()\n",
    "                article = g.extract(raw_html=article_html)\n",
    "\n",
    "                row_id = shafaqna['Document ID'].iloc[i]\n",
    "                article_data = {\n",
    "                    'row_id': row_id,\n",
    "                    'data': {\n",
    "                        'meta': {'meta_description': article.meta_description, 'url': url},\n",
    "                        'image': article.top_image.src if article.top_image else None,\n",
    "                        'domain': article.domain,\n",
    "                        'title': article.title,\n",
    "                        'cleaned_text': article.cleaned_text,\n",
    "                        'opengraph': article.opengraph,\n",
    "                        'tags': article.tags,\n",
    "                        'tweets': article.tweets,\n",
    "                        'movies': article.movies,\n",
    "                        'links': article.links,\n",
    "                        'authors': article.authors,\n",
    "                        'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                    }\n",
    "                }\n",
    "                content.append(article_data)\n",
    "            else:\n",
    "                print(f\"No suitable content found in the url number {i} : {url}\")\n",
    "                \n",
    "                content.append(article_none)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur pour {url}: {e}\")\n",
    "            # En cas d'erreur de requ√™te, ajouter un article vide\n",
    "            \n",
    "            content.append(article_none)\n",
    "\n",
    "        # D√©lai pour √©viter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return content, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a39550",
   "metadata": {},
   "source": [
    "### Alriyadh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d389e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url_no_verify(url: str):\n",
    "    \"\"\"Ouvre une URL sans v√©rifier le certificat SSL (√† √©viter en production).\"\"\"\n",
    "    context = ssl._create_unverified_context()\n",
    "    return urlopen(url, context=context)\n",
    "\n",
    "def alriyadh_scraper(alri):\n",
    "    contents_alri, raw_content = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(alri['URL']), desc=\"Processing Alriyadh URLs\"):\n",
    "        try:\n",
    "            response = open_url_no_verify(url)\n",
    "            html = response.read().decode('utf-8', errors='ignore')\n",
    "            raw_content.append(html)\n",
    "            \n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            title = soup.find('div', class_='article-title').text if soup.find('div', class_='article-title') else 'Titre non trouv√©'\n",
    "            text = soup.find('div', class_='article-text').text if soup.find('div', class_='article-text') else 'Texte non trouv√©'\n",
    "            \n",
    "            row_id = alri['Document ID'].iloc[i]\n",
    "\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,  # √Ä ajuster si une image est pr√©sente sur la page\n",
    "                    'domain': None,  # Vous pouvez extraire le domaine si n√©cessaire\n",
    "                    'title': title,\n",
    "                    'cleaned_text': text,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Vous pouvez extraire la date de publication si n√©cessaire\n",
    "                }\n",
    "            }\n",
    "            contents_alri.append(article_data)  \n",
    "\n",
    "        except (HTTPError, URLError, ssl.SSLError, ValueError) as e:\n",
    "            print(f\"Erreur pour {url} : {e}\")\n",
    "            # Pour ne pas d√©caler les id, on rajoute quand m√™me des valeurs par d√©faut (None)\n",
    "            row_id = alri['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents_alri.append(article_data)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return contents_alri, raw_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca413bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alri = data_false[data_false['Source Domain'] == 'alriyadh.com']\\ncontents_alri = alriyadh_scraper(alri[:5])\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"alri = data_false[data_false['Source Domain'] == 'alriyadh.com']\n",
    "contents_alri = alriyadh_scraper(alri[:5])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11659a",
   "metadata": {},
   "source": [
    "### sahafaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92d2fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pas possible en requests, pas d'appel api  ==> selenium\n",
    "# Ne marche pas avec goose .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a95fb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Liste des classes possibles √† tester dans des <div> -> agregateur donc pas forcement tout le tps la meme structure\n",
    "    class_candidates = [\n",
    "        'text-body',\n",
    "        'story_text',\n",
    "        'post--content',\n",
    "        'post_details_inner',\n",
    "        \"news-content\",\n",
    "        \"news_text\",\n",
    "        \"article\",\n",
    "        \"description-text\",\n",
    "        'col-md-12',\n",
    "    ]\n",
    "\n",
    "    content_parts = []\n",
    "\n",
    "    for class_name in class_candidates:\n",
    "        if class_name == 'article':\n",
    "            found = soup.find_all('div', id=lambda c: c and 'article' in c)  # seul cas ou le texte est dans un div avec id (sinon class)\n",
    "        else:\n",
    "            found = soup.find_all('div', class_=lambda c: c and class_name in c)\n",
    "\n",
    "        if found and str(404) not in str(found):\n",
    "            for f in found:\n",
    "                text = f.get_text(strip=True)\n",
    "                if len(text) > 100:  # Ajout d'un contr√¥le sur la taille du texte\n",
    "                    content_parts.append(text)\n",
    "\n",
    "            if content_parts:\n",
    "                return {\n",
    "                    'content': \"##\".join(content_parts),\n",
    "                    'Title': soup.find('meta', property='og:title')['content'] if soup.find('meta', property='og:title') else \"Titre non trouv√©\",\n",
    "                    'URL': url\n",
    "                }\n",
    "\n",
    "    print(f\"Aucun contenu trouv√© dans les classes sp√©cifi√©es pour l'url {url} ... \\n On cherche dans le og:description\")\n",
    "    meta = soup.find('meta', property='og:description')\n",
    "    if meta and meta.get('content') and '[‚Ä¶]' not in meta['content']:\n",
    "        return {\n",
    "            'content': meta['content'],\n",
    "            'title': soup.find('meta', property='og:title')['content'] if soup.find('meta', property='og:title') else \"Titre non trouv√©\",\n",
    "            'url': url\n",
    "        }\n",
    "\n",
    "    print(f\"##### Aucun contenu trouv√© pour l'url {url} ###### \\n\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def sahaafa_scraper(sahafaa):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--ignore-ssl-errors=yes')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    # options.add_argument('--headless')  \n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "\n",
    "    contents, raw_content = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(sahafaa['URL'].tolist()), desc=\"Processing sahaafa.net URLs\"):\n",
    "        row_id = sahafaa['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Essayer de passer dans un iframe si pr√©sent\n",
    "            try:\n",
    "                iframe = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"iframe\"))\n",
    "                )\n",
    "                driver.switch_to.frame(iframe)\n",
    "            except Exception as e:\n",
    "                contents.append(article_none)\n",
    "                print(f\"[{url}] Pas d'iframe trouv√© ou erreur :\", e)\n",
    "\n",
    "            html = driver.page_source\n",
    "            raw_content.append(html)\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            elements = extract_article_text(html, url)\n",
    "\n",
    "            # R√©cup√©rer l'ID du document\n",
    "            row_id = sahafaa['Document ID'].iloc[i]\n",
    "\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': elements['URL']},\n",
    "                    'image': None,  # Vous pouvez extraire une image si n√©cessaire\n",
    "                    'domain': None,  # Vous pouvez extraire le domaine si n√©cessaire\n",
    "                    'title': elements[\"Title\"],\n",
    "                    'cleaned_text': elements[\"content\"],\n",
    "                    'opengraph': {},  # Vous pouvez compl√©ter selon votre besoin\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None  # Vous pouvez extraire la date de publication si n√©cessaire\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "            driver.switch_to.default_content()  # Revenir au contenu principal\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{url}] Erreur g√©n√©rale :\", e)\n",
    "            # Pour ne pas d√©caler les ID, on rajoute quand m√™me des valeurs par d√©faut (None)\n",
    "            contents.append(article_none)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0baf8",
   "metadata": {},
   "source": [
    "### klyoum.com\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11709d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def klyoum_scraper(klyoum):\n",
    "    g = Goose()\n",
    "    contents, raw_content = [], []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "        'Referer': 'https://www.msn.com/',\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(klyoum['URL'].tolist()),desc='processing klyoum URLs'):\n",
    "        row_id = klyoum['Document ID'].iloc[i]\n",
    "        \n",
    "        # Article par d√©faut en cas d'erreur pour ne pas d√©caler les ids\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            contents.append(article_none)\n",
    "            raw_content.append(None)\n",
    "            continue\n",
    "        \n",
    "        raw_content.append(response.text)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        button = soup.find('a', attrs={\n",
    "            'onclick': \"gtag('event', 'click', {'event_category': 'link to original website','event_label': 'button_link to original website'});\"\n",
    "        })\n",
    "\n",
    "        if button and button.get('href'):\n",
    "            href = button.get('href')\n",
    "            params = parse_qs(urlparse(href).query)\n",
    "            domain = params.get('cc', [''])[0]\n",
    "            title = params.get('t', [''])[0]\n",
    "            \n",
    "            if domain and title:\n",
    "                params = {\n",
    "                    \"api_key\": \"77c5ebb6d414ddeb58b73c1eccddfcddedb5ce0996bb1a68f5bbd419513d8088\", #laura's one\n",
    "                    \"engine\": \"google\",\n",
    "                    \"hl\": \"fr\",\n",
    "                    \"q\": f'{str(domain)} {str(title)}'  # La vraie requ√™te ici\n",
    "                }\n",
    "                search = GoogleSearch(params)\n",
    "                data = search.get_dict()\n",
    "\n",
    "                try:\n",
    "                    links = data['organic_results']\n",
    "                except KeyError as e:\n",
    "                    contents.append(article_none)\n",
    "                    print(f\"No organic value in the serpapi result for {url}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    if links:\n",
    "                        first_link = links[0][\"link\"]\n",
    "                        article = g.extract(first_link)\n",
    "                        print(\"daaate\", article.publish_date)\n",
    "                        article_data = {\n",
    "                            'row_id': row_id,\n",
    "                            'data': {\n",
    "                                'meta': {'meta_description': article.meta_description, 'url': first_link},\n",
    "                                'image': article.top_image.src if article.top_image else None,\n",
    "                                'domain': article.domain,\n",
    "                                'title': article.title,\n",
    "                                'cleaned_text': article.cleaned_text,\n",
    "                                'opengraph': article.opengraph,\n",
    "                                'tags': article.tags,\n",
    "                                'tweets': article.tweets,\n",
    "                                'movies': article.movies,\n",
    "                                'links': article.links,\n",
    "                                'authors': article.authors,\n",
    "                                'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                            }\n",
    "                        }\n",
    "                        contents.append(article_data)\n",
    "                    else:\n",
    "                        print(\"No Google result found for the URL.\")\n",
    "                        contents.append(article_none)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Google Search failed for {url}: {e}\")\n",
    "                    contents.append(article_none)\n",
    "            else:\n",
    "                print(f\"Missing parameters in href for {url}.\")\n",
    "                contents.append(article_none)\n",
    "        else:\n",
    "            print(f\"No button found for {url}.\")\n",
    "            contents.append(article_none)\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df989dca",
   "metadata": {},
   "source": [
    "### SPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "266af4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = data_false[data_false['Source Domain'] == 'spa.gov.sa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f65f784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spa_scraper(spa, timeout=150):\n",
    "    contents, raw_content = [], []\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0',\n",
    "        'Referer': 'https://www.msn.com/',\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(spa['URL'].tolist()), desc=\"Processing spa URLs\"):\n",
    "        row_id = spa['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            # Effectuer la requ√™te HTTP\n",
    "            response = requests.get(url, timeout=timeout, headers=headers)\n",
    "            raw_content.append(response.text)\n",
    "            \n",
    "            # Analyser la page avec BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Extraction du contenu de l'article avec Goose\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=response.text)\n",
    "            \n",
    "            title = article.title\n",
    "            meta = article.meta_description\n",
    "            \n",
    "            # R√©cup√©rer le contenu √† partir de la balise og:description\n",
    "            content = soup.find(\"meta\", property=\"og:description\")['content'] if soup.find(\"meta\", property=\"og:description\") else \"\"\n",
    "            if content == \"\":\n",
    "                print(f\"Aucun contenu trouv√© dans la balise og:description pour l'URL : {url}\")\n",
    "                continue  # Passer √† l'URL suivante si aucun contenu n'est trouv√©\n",
    "            \n",
    "            # Cr√©er un dictionnaire de donn√©es √† ajouter √† `contents`\n",
    "            fields = [\n",
    "                'meta', 'image', 'domain', 'title', 'cleaned_text', 'opengraph',\n",
    "                'tags', 'tweets', 'movies', 'links', 'authors', 'publish_date'\n",
    "            ]\n",
    "            d = {field: \"\" for field in fields}\n",
    "            d['title'] = title\n",
    "            d['cleaned_text'] = content\n",
    "            d['meta'] = meta\n",
    "            \n",
    "            # R√©cup√©rer l'ID du document\n",
    "            row_id = spa['Document ID'].iloc[i]\n",
    "            \n",
    "            # Ajouter les donn√©es de l'article √† la liste `contents`\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': d\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "            print(f\"Contenu extrait pour {url}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nErreur de requ√™te pour {url}: {e}\")\n",
    "            # Pour ne pas d√©caler les ID, on rajoute quand m√™me des valeurs par d√©faut (None)\n",
    "            \n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Gestion d'autres erreurs\n",
    "            print(f\"Erreur d'extraction pour {url}: {e}\")\n",
    "            \n",
    "            contents.append(article_none)\n",
    "\n",
    "        # Ajouter un d√©lai pour √©viter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ddc19",
   "metadata": {},
   "source": [
    "### aawsat.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6f434a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aawsat_scraper(aawsat):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "    contents, raw_contents = [], []\n",
    "    \n",
    "    for i, url in tqdm(enumerate(aawsat['URL']), desc=\"Processing aawsat URLs\"):\n",
    "        row_id = aawsat['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id': row_id,\n",
    "            'data': {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            raw_contents.append(response.text)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                g = Goose()\n",
    "                article = g.extract(raw_html=response.text)\n",
    "                row_id = aawsat['Document ID'].iloc[i]\n",
    "                \n",
    "                article_data = {\n",
    "                    'row_id': row_id,\n",
    "                    'data': {\n",
    "                        'meta': {'meta_description': article.meta_description, 'url': url},\n",
    "                        'image': article.top_image.src if article.top_image else None,\n",
    "                        'domain': article.domain,\n",
    "                        'title': article.title,\n",
    "                        'cleaned_text': article.cleaned_text,\n",
    "                        'opengraph': article.opengraph,\n",
    "                        'tags': article.tags,\n",
    "                        'tweets': article.tweets,\n",
    "                        'movies': article.movies,\n",
    "                        'links': article.links,\n",
    "                        'authors': article.authors,\n",
    "                        'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                    }\n",
    "                }\n",
    "                contents.append(article_data)\n",
    "\n",
    "            else:\n",
    "                print(f\"Acc√®s √©chou√© √† : {url} avec le code {response.status_code}\")\n",
    "                # Pour ne pas d√©caler les ID, on rajoute quand m√™me None\n",
    "                \n",
    "                contents.append(article_none)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement de {url}: {e}\")\n",
    "            \n",
    "            contents.append(article_none)\n",
    "\n",
    "        # Ajout d'un d√©lai pour √©viter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return contents, raw_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f40ff5",
   "metadata": {},
   "source": [
    "### RT.com & NYT & CNN & Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60f804e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = data_false[data_false['Source Domain'] == 'rt.com']\n",
    "nytimes = data_false[data_false['Source Domain'] == 'nytimes.com']\n",
    "cnn = data_false[data_false['Source Domain'] == 'cnn.com']\n",
    "yahoo = data_false[data_false['Source Domain'] == 'yahoo.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "986bd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_nyt_cnn_scraper(rt):\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    contents,raw_contents = [],[]\n",
    "    for i,url in tqdm(enumerate(rt['URL'].tolist()), desc=\"Processing rt/nyt/cnn URLs\"):\n",
    "        row_id = rt['Document ID'].iloc[i]\n",
    "        article_none = {\n",
    "            'row_id' : row_id,\n",
    "            'data' : {\n",
    "            'meta': {'canonical': url},\n",
    "            'image': None,\n",
    "            'domain': None,\n",
    "            'title': None,\n",
    "            'cleaned_text': None,\n",
    "            'opengraph': {},\n",
    "            'tags': [],\n",
    "            'tweets': [],\n",
    "            'movies': [],\n",
    "            'links': [],\n",
    "            'authors': [],\n",
    "            'publish_date': None\n",
    "        }}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers= headers)\n",
    "            response.raise_for_status()  \n",
    "            raw_contents.append(response.text)\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=response.text)\n",
    "            row_id = rt['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id' : row_id,\n",
    "                'data' : {\n",
    "                    'meta': article.meta_description,\n",
    "                    'image': article.top_image.src if article.top_image else None,\n",
    "                    'domain': article.domain,\n",
    "                    'title': article.title,\n",
    "                    'cleaned_text': article.cleaned_text,\n",
    "                    'opengraph': article.opengraph,\n",
    "                    'tags': article.tags,\n",
    "                    'tweets': article.tweets,\n",
    "                    'movies': article.movies,\n",
    "                    'links': article.links,\n",
    "                    'authors': article.authors,\n",
    "                    'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nErreur de requ√™te pour {url} : {e}\")\n",
    "            #pour ne pas d√©caler les id on rajotue quand m√™me None\n",
    "            \n",
    "            contents.append(article_none)\n",
    "        except Exception as e:\n",
    "            #pour ne pas d√©caler les id on rajotue quand m√™me None\n",
    "            contents.append(article_none)\n",
    "            print(f\"\\nErreur d'extraction pour {url} : {e}\")\n",
    "\n",
    "        time.sleep(0.5) \n",
    "\n",
    "    return contents,raw_contents\n",
    "\n",
    "\n",
    "#content = rt_nyt_cnn_scraper(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694746c",
   "metadata": {},
   "source": [
    "### Investing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c11e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "investing = data_false[data_false['Source Domain'] == 'investing.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5007781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investing_scraper(invest):\n",
    "    contents, raw_content = [], []\n",
    "    \n",
    "    # Initialisation des options de Chrome\n",
    "    options = webdriver.ChromeOptions()  # Pas de headless sinon premi√®re attente ne fonctionne pas\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--ignore-ssl-errors=yes')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "\n",
    "    for i,url in tqdm(enumerate(invest['URL'].tolist())):\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        try:\n",
    "            driver.get(url)\n",
    "\n",
    "            # Attendre que le titre de la page soit charg√©, il y a un d√©lai de 3s pour charger la page \n",
    "            WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.TAG_NAME, \"title\")))\n",
    "\n",
    "            raw_content.append(driver.page_source)\n",
    "            element = driver.find_element(By.TAG_NAME, 'h1')\n",
    "\n",
    "            html = driver.page_source\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=html)\n",
    "            row_id = invest['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data':{\n",
    "                'meta': article.meta_description,\n",
    "                'image': article.top_image.src if article.top_image else None,\n",
    "                'domain': article.domain,\n",
    "                'title': article.title,\n",
    "                'cleaned_text': article.cleaned_text,\n",
    "                'opengraph': article.opengraph,\n",
    "                'tags': article.tags,\n",
    "                'tweets': article.tweets,\n",
    "                'movies': article.movies,\n",
    "                'links': article.links,\n",
    "                'authors': article.authors,\n",
    "                'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "            }}\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur est survenue pour l'URL {url}: {e}\")\n",
    "            #pour ne pas d√©caler les id on rajotue quand m√™me None\n",
    "            row_id = invest['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id' : row_id,\n",
    "                'data' : {\n",
    "                'meta': {'canonical': url},\n",
    "                'image': None,\n",
    "                'domain': None,\n",
    "                'title': None,\n",
    "                'cleaned_text': None,\n",
    "                'opengraph': {},\n",
    "                'tags': [],\n",
    "                'tweets': [],\n",
    "                'movies': [],\n",
    "                'links': [],\n",
    "                'authors': [],\n",
    "                'publish_date': None\n",
    "            }}\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        finally:\n",
    "            driver.quit() #obliger de le fermer √† chaque it \n",
    "\n",
    "    return contents, raw_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e2d5b",
   "metadata": {},
   "source": [
    "### Sohu.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b15b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sohu = data_false[data_false['Source Domain'] == 'sohu.com']\n",
    "#goose ne marche pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c641d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sohu_scraper(sohu):\n",
    "    contents, raw_contents = [], []\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    appended = False\n",
    "    for i, url in tqdm(enumerate(sohu['URL'].tolist()), desc=\"Processing sohu URLs\"):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            raw_contents.append(response.text)\n",
    "            appended = True\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # R√©cup√©ration du titre\n",
    "            title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
    "\n",
    "            # Extraction du contenu de l'article\n",
    "            content_div = soup.find('div', attrs={'data-spm': 'content'})\n",
    "            content = content_div.get_text(strip=True, separator=\"\\n\").strip() if content_div else \"\"\n",
    "\n",
    "            row_id = sohu['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': title,\n",
    "                    'cleaned_text': content,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur de requ√™te pour {url} : {e}\")\n",
    "            # Pour ne pas d√©caler les ID, on ajoute quand m√™me un article avec des valeurs par d√©faut (None)\n",
    "            row_id = sohu['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            if not appended : raw_contents.append(None)\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur d'extraction pour {url} : {e}\")\n",
    "            # Pour ne pas d√©caler les ID, on ajoute quand m√™me un article avec des valeurs par d√©faut (None)\n",
    "            row_id = sohu['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            if not appended : raw_contents.append(None)\n",
    "            contents.append(article_data)\n",
    "\n",
    "        # Ajout d'un d√©lai pour √©viter de surcharger le serveur\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return contents, raw_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42cf19",
   "metadata": {},
   "source": [
    "### Goal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3648512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_scraper(goal):\n",
    "    contents, raw_content = [], []\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    for i, url in tqdm(enumerate(goal['URL'].tolist()), desc=\"Processing goal.com URLs\"):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            raw_content.append(response.text)\n",
    "\n",
    "            g = Goose()\n",
    "            article = g.extract(raw_html=response.text)\n",
    "\n",
    "            row_id = goal['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': article.meta_description,\n",
    "                    'image': article.top_image.src if article.top_image else None,\n",
    "                    'domain': article.domain,\n",
    "                    'title': article.title,\n",
    "                    'cleaned_text': article.cleaned_text,\n",
    "                    'opengraph': article.opengraph,\n",
    "                    'tags': article.tags,\n",
    "                    'tweets': article.tweets,\n",
    "                    'movies': article.movies,\n",
    "                    'links': article.links,\n",
    "                    'authors': article.authors,\n",
    "                    'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                }\n",
    "            }\n",
    "\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nErreur de requ√™te pour {url} : {e}\")\n",
    "            # Pour ne pas d√©caler les ID, on ajoute un article avec des valeurs par d√©faut (None)\n",
    "            row_id = goal['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Gestion des autres erreurs\n",
    "            print(f\"\\nErreur d'extraction pour {url} : {e}\")\n",
    "            # M√™me traitement que pour les erreurs de requ√™te\n",
    "            row_id = goal['Document ID'].iloc[i]\n",
    "            article_data = {\n",
    "                'row_id': row_id,\n",
    "                'data': {\n",
    "                    'meta': {'canonical': url},\n",
    "                    'image': None,\n",
    "                    'domain': None,\n",
    "                    'title': None,\n",
    "                    'cleaned_text': None,\n",
    "                    'opengraph': {},\n",
    "                    'tags': [],\n",
    "                    'tweets': [],\n",
    "                    'movies': [],\n",
    "                    'links': [],\n",
    "                    'authors': [],\n",
    "                    'publish_date': None\n",
    "                }\n",
    "            }\n",
    "            contents.append(article_data)\n",
    "\n",
    "        time.sleep(0.5)  # D√©lai pour √©viter de surcharger le serveur\n",
    "\n",
    "    return contents, raw_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1624d",
   "metadata": {},
   "source": [
    "## Gather all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "270ea45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lecture du df meltwater, pour chaque url soit on scrapp normal (selenium) soit on scrapped avec la fonction ad&quate en fonction de la source \n",
    "# Scrapping puis enregistrement dans /content (pour le fichier source) et /clean_content (pour le json output)\n",
    "# Chaque algo de scrap manuel renvoie une liste de json correspnod √† chacun de ses articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e454d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self, data, manualy_scrapped_sources):\n",
    "        self.data = data\n",
    "        self.manualy_scrapped_sources = manualy_scrapped_sources\n",
    "        self.source_domains = data['Source Domain'].unique()\n",
    "        self.minister = None\n",
    "        \n",
    "\n",
    "        sys.stdout = open('output.log', 'w')\n",
    "        print(\"Everything from now on goes to the log file.\")\n",
    "\n",
    "    def route_scrap(self):\n",
    "        # Supprimer les dossiers 'content' et 'cleaned_content' s'ils existent\n",
    "        content_folder = 'content'\n",
    "        cleaned_content_folder = 'cleaned_content'\n",
    "        \n",
    "        if os.path.isdir(content_folder):\n",
    "            shutil.rmtree(content_folder)  # Supprimer le dossier 'content'\n",
    "            print(f\"Le dossier {content_folder} a √©t√© supprim√©.\")\n",
    "        \n",
    "        if os.path.isdir(cleaned_content_folder):\n",
    "            shutil.rmtree(cleaned_content_folder)  # Supprimer le dossier 'cleaned_content'\n",
    "            print(f\"Le dossier {cleaned_content_folder} a √©t√© supprim√©.\")\n",
    "        \n",
    "        # Traiter chaque source\n",
    "        for source in self.source_domains:\n",
    "            source_data = self.data[self.data['Source Domain'] == source]\n",
    "\n",
    "            if source in self.manualy_scrapped_sources:\n",
    "                self.manual_scrap(source_data)\n",
    "            else:\n",
    "                self.default_scrap(source_data)\n",
    "\n",
    "            # Lib√©rer la m√©moire en supprimant les donn√©es d√©j√† trait√©es\n",
    "            self.data = self.data[self.data['Source Domain'] != source].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \n",
    "    def check_condition(self, article):\n",
    "        if list(article.keys()) == ['row_id','data'] : \n",
    "            article = article['data']\n",
    "        try : \n",
    "            if not article : \n",
    "                print(\"not article\")\n",
    "            if len(article['cleaned_text'])<50:\n",
    "                return False\n",
    "            #if 'I been blocked' in str(article[\"infos\"]):\n",
    "            #    return False\n",
    "            if 'changement de r√©seau' in str(article['cleaned_text']):\n",
    "                return False\n",
    "            if 'cloudflare' in str(article['title']):\n",
    "                return False\n",
    "            #if 'cloudflare' in str(article.infos):\n",
    "            #    return False\n",
    "            if 'cloudflare' in str(article['cleaned_text']):\n",
    "                return False\n",
    "            if '404 Error' in str(article['title']):\n",
    "                return False\n",
    "            return True\n",
    "        except : \n",
    "            print(\"Article non valide pour le check conditions\")\n",
    "            return False\n",
    "            \n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "    ################# Use of manual scraping, coded above ##########################\n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    def manual_scrap(self, source_data):\n",
    "        \"\"\"\n",
    "        Nos scrapers manuels renvoient deux objets : content (qui est une liste de clean json) et raw content (une liste de fichiers .txt de page source)\n",
    "        A partir de cela on peut save les articles dans content et /clean_content avec l'id row fourni dans le \"content\" \n",
    "        \n",
    "        ####\n",
    "        content.keys() = [row_id, data] o√π row_id est l'id du document et data est le json d'int√©r√™t \n",
    "        type(raw_content) = .txt\n",
    "        \"\"\"\n",
    "        source_name = source_data['Source Domain'].iloc[0]\n",
    "        print(f\"Manual scraping for {source_name}\")\n",
    "        if source_name == 'msn.com' :\n",
    "            content, raw_content = msn_scraper(source_data)\n",
    "        elif source_name == 'sauress.com' :\n",
    "            content,raw_content = sauress_scraper(source_data)\n",
    "        elif source_name == 'aawsat.com' :\n",
    "            content,raw_content = aawsat_scraper(source_data)\n",
    "        elif source_name == 'alriyadh.com' :\n",
    "            content,raw_content = alriyadh_scraper(source_data)\n",
    "        elif source_name == 'klyoum.com' :\n",
    "            content,raw_content = klyoum_scraper(source_data)\n",
    "        elif source_name == 'spa.gov.sa' :\n",
    "            content,raw_content = spa_scraper(source_data)\n",
    "        elif source_name == 'rt.com' or source_name == 'nytimes.com' or source_name == 'cnn.com' or source_name == 'yahoo.com' :\n",
    "            content,raw_content = rt_nyt_cnn_scraper(source_data)\n",
    "        elif source_name == 'shafaqna.com' :\n",
    "            content,raw_content = shafaqna_scraper(source_data)\n",
    "        elif source_name == 'goal.com' :\n",
    "            content,raw_content = goal_scraper(source_data)\n",
    "        elif source_name == 'sohu.com' :\n",
    "            content,raw_content = sohu_scraper(source_data)\n",
    "        elif source_name == 'investing.com' :\n",
    "            content,raw_content = investing_scraper(source_data) \n",
    "        else : \n",
    "            raise ValueError(\"Source Domain non valide pour le manual scrapping\")\n",
    "        \n",
    "        self.minister = source_data['RB Minister'].iloc[0] #on suppose que le minister est le m√™me dans tout le batch de donn√©es \n",
    "        # Sauvegarder le contenu brut et nettoy√©\n",
    "        self.manual_save_content(raw_content = raw_content, content = content)\n",
    "        self.manual_save_clean_content(cleaned_content = content)\n",
    "    \n",
    "    def manual_save_content(self, raw_content, content):\n",
    "        \"\"\"\n",
    "        Sauvegarder le contenu brut pour une source sp√©cifique\n",
    "        `raw_content` = liste de fichiers .txt \n",
    "        `content` = liste de json o√π chaque article contient une cl√© 'row_id'\n",
    "        \"\"\"\n",
    "        \n",
    "        folder_path =f\"content/{self.minister}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)  \n",
    "        for i, article in enumerate(raw_content):  # Sauvegarder chaque fichier .txt de raw_content\n",
    "            row_id = content[i]['row_id']  # Utiliser 'row_id' du contenu\n",
    "            file_name = f\"{row_id}.txt\"  # Nom du fichier bas√© sur 'row_id'\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(article)\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def manual_save_clean_content(self, cleaned_content):\n",
    "        \"\"\"\n",
    "        Sauvegarder le contenu nettoy√© pour une source sp√©cifique au format JSON, fait automatiquement normalement par les algo de scrap manuels\n",
    "        \"\"\"\n",
    "        folder_path = f\"cleaned_content/{self.minister}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)  \n",
    "\n",
    "        for i, article in enumerate(cleaned_content):  \n",
    "            if not self.check_condition(article=article) :  #possible que le clean json soit vide (champs vides)\n",
    "                print(f\"Article {article} does not meet the cleaning criteria.\")\n",
    "                continue\n",
    "            row_id = article['row_id'] \n",
    "            file_name = f\"{row_id}_cleaned.json\" \n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            cleaned_data = article['data']#on ne veux pas row_id \n",
    "\n",
    "            try:\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "                    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=4)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving cleaned content for {file_name}: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "    ####### Default Scrapping with firas's method with selenium and goose ##########\n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "    \n",
    "\n",
    "    def default_save_content(self, source_data, max_retries=2):\n",
    "        \"\"\"Appel pour chaque source, et renvoie le folder dans lequel il a enregistrer les contents (utile dans le cas o√π l'on a plusieurs ministre)\n",
    "        et le nombre d'articles appended, pour ensuite clean seulement ceux qui nous interesse et pas parcourir tout le folder /content\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--ignore-ssl-errors=yes')\n",
    "        options.add_argument('--ignore-certificate-errors')\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(30)  # Timeout pour le chargement de la page\n",
    "\n",
    "        article_saved = 0\n",
    "        folder_path = 'Error'\n",
    "        if len(source_data) == 0:\n",
    "            print(\"Source data is empty. Exiting.\")\n",
    "            driver.quit()\n",
    "            return folder_path, article_saved\n",
    "        for index, row in tqdm(source_data.iterrows(), total=source_data.shape[0]):\n",
    "            \n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "\n",
    "\n",
    "                try:\n",
    "                    driver.get(row[\"URL\"])\n",
    "\n",
    "                    time.sleep(3)  # Attendre quelques secondes pour que la page se charge\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                    self.minister = row['RB Minister']\n",
    "                    folder_path = f\"content/{self.minister}\"\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                    # Sauvegarder le contenu de la page dans un fichier texte\n",
    "                    with open(f\"{folder_path}/{row['Document ID']}.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "                        text_file.write(str(soup))\n",
    "\n",
    "                    article_saved +=1\n",
    "                    break  # Si tout s'est bien pass√©, sortir de la boucle de r√©essai\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {row['URL']}: {e}\")\n",
    "                    retries += 1\n",
    "                    print(f\"Tentative {retries} sur {max_retries} pour {row['URL']}\")\n",
    "                    time.sleep(3) \n",
    "\n",
    "                    if retries == max_retries:\n",
    "                        print(f\"√âchec apr√®s {max_retries} tentatives pour {row['URL']}\")\n",
    "                        folder_path = \"Error\"\n",
    "                        break  # Passer √† l'URL suivante apr√®s le nombre maximum de tentatives\n",
    "                        \n",
    "\n",
    "        driver.quit()\n",
    "        return folder_path, article_saved\n",
    "    \n",
    "\n",
    "    def default_save_clean_content(self, content_folder_path, n):\n",
    "        \"\"\"On r√©cupere les n derniers fichiers saved par la fonction save_content puis on v√©rifie si ils peuvent √™tre clean√©.\n",
    "        Si oui on le save, sinon on le passe. \"\"\"\n",
    "        \n",
    "        if not os.path.isdir(content_folder_path):\n",
    "            print(f'Error: {content_folder_path} does not exist or is not a directory.')\n",
    "            return  #on a aucun article √† clean puisque le content_folder == Error ou n'existe pas \n",
    "\n",
    "        g = Goose()\n",
    "        results = []\n",
    "\n",
    "        for filename in tqdm(os.listdir(content_folder_path)[-n:]):  # On s'int√©resse seulement aux n derniers articles sauvegard√©s\n",
    "            \n",
    "            file_path = os.path.join(content_folder_path, filename)\n",
    "\n",
    "            if not os.path.isfile(file_path):\n",
    "                print(f'le file path : {file_path} n\\'est pas un file path valide')\n",
    "                continue  \n",
    "                \n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    html_raw = file.read()\n",
    "                article = g.extract(raw_html=html_raw)\n",
    "                if type(article.publish_date) == list:\n",
    "                    article.publish_date = article.publish_date[0]\n",
    "                    print(\"conversion date de liste √† date\")\n",
    "                cleaned_data = {\n",
    "                    'meta': article.meta_description,\n",
    "                    'image': article.top_image.src if article.top_image else None,\n",
    "                    'domain': article.domain,\n",
    "                    'title': article.title,\n",
    "                    'cleaned_text': article.cleaned_text,\n",
    "                    'opengraph': article.opengraph,\n",
    "                    'tags': article.tags,\n",
    "                    'tweets': article.tweets,\n",
    "                    'movies': article.movies,\n",
    "                    'links': article.links,\n",
    "                    'authors': article.authors,\n",
    "                    'publish_date': str(article.publish_date) if article.publish_date else None\n",
    "                }\n",
    "\n",
    "                if not self.check_condition(article=cleaned_data): \n",
    "                    print(f'le fichier {cleaned_data}.txt ne r√©pond pas aux crit√®res de cleaning ...... ')\n",
    "                    continue  \n",
    "\n",
    "                \n",
    "\n",
    "                document_id = os.path.splitext(filename)[0]  # Extraire l'id avant le .txt\n",
    "                \n",
    "                output_folder = f\"cleaned_content/{self.minister}\"\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                output_file = os.path.join(output_folder, f\"{document_id}_clean.json\")\n",
    "\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "                    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")  # Afficher l'erreur si une exception se produit pendant le traitement\n",
    "                continue\n",
    "\n",
    "        print(\"Cleaning process completed.\")\n",
    "\n",
    "\n",
    "    def default_scrap(self, source_data):\n",
    "        '''Main fonction pour scrapper des sources de base : on append tous les articles.txt de la source dans /content puis on s'occupe de les cleans'''\n",
    "        content_folder_path,n = self.default_save_content(source_data)\n",
    "        self.default_save_clean_content(content_folder_path,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c195954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"20250428_Reema_Bandar_Al_Saud_Meltwater raw data (2024).xlsx\")\n",
    "scrapped_article = pd.read_excel(\"12-05-2025-SourceDomainReport.xlsx\")\n",
    "manualy_scraped_sources = scrapped_article[scrapped_article['Scrapped'] == True]['Source Domain'].tolist()\n",
    "unscrappable = scrapped_article[scrapped_article['Scrapped'] == False]['Source Domain'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fa6841ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RB Minister'] = [\"Reema_Bandar\"] * len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data[\"Source Domain\"].isin(unscrappable)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b7efccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.05s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.92it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:22<00:00,  3.76s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.07it/s]\n",
      "Processing Alriyadh URLs: 106it [01:15,  1.41it/s]\n",
      "processing klyoum URLs: 60it [02:13,  2.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m scraper \u001b[38;5;241m=\u001b[39m Scraper(data \u001b[38;5;241m=\u001b[39mdata,manualy_scrapped_sources\u001b[38;5;241m=\u001b[39mmanualy_scraped_sources)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute_scrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[140], line 30\u001b[0m, in \u001b[0;36mScraper.route_scrap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m source_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource Domain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m source]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanualy_scrapped_sources:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_scrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_scrap(source_data)\n",
      "Cell \u001b[0;32mIn[140], line 92\u001b[0m, in \u001b[0;36mScraper.manual_scrap\u001b[0;34m(self, source_data)\u001b[0m\n\u001b[1;32m     90\u001b[0m     content,raw_content \u001b[38;5;241m=\u001b[39m alriyadh_scraper(source_data)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mklyoum.com\u001b[39m\u001b[38;5;124m'\u001b[39m :\n\u001b[0;32m---> 92\u001b[0m     content,raw_content \u001b[38;5;241m=\u001b[39m \u001b[43mklyoum_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspa.gov.sa\u001b[39m\u001b[38;5;124m'\u001b[39m :\n\u001b[1;32m     94\u001b[0m     content,raw_content \u001b[38;5;241m=\u001b[39m spa_scraper(source_data)\n",
      "Cell \u001b[0;32mIn[144], line 60\u001b[0m, in \u001b[0;36mklyoum_scraper\u001b[0;34m(klyoum)\u001b[0m\n\u001b[1;32m     53\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m77c5ebb6d414ddeb58b73c1eccddfcddedb5ce0996bb1a68f5bbd419513d8088\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#laura's one\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(domain)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(title)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# La vraie requ√™te ici\u001b[39;00m\n\u001b[1;32m     58\u001b[0m }\n\u001b[1;32m     59\u001b[0m search \u001b[38;5;241m=\u001b[39m GoogleSearch(params)\n\u001b[0;32m---> 60\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     links \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morganic_results\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:103\u001b[0m, in \u001b[0;36mSerpApiClient.get_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m        Dict with the formatted response content\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m        (alias for get_dictionary)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:96\u001b[0m, in \u001b[0;36mSerpApiClient.get_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dictionary\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        Dict with the formatted response content\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:83\u001b[0m, in \u001b[0;36mSerpApiClient.get_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Formatted JSON search results using json package\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:70\u001b[0m, in \u001b[0;36mSerpApiClient.get_results\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/search\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns:\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m        Response text field\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/serpapi/serp_api_client.py:59\u001b[0m, in \u001b[0;36mSerpApiClient.get_response\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     57\u001b[0m     url, parameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_url(path)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# print(url)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/envFormationP2/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scraper = Scraper(data =data,manualy_scrapped_sources=manualy_scraped_sources)\n",
    "scraper.route_scrap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envFormationP2)",
   "language": "python",
   "name": "envformationp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
