{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HugoKD/NLP/blob/main/Projet2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOphPSpgEk7Y"
      },
      "source": [
        "### Autocomplétion et génération de phrases avec des modèles de langue n-grammes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsyvFj-NEk7a"
      },
      "source": [
        "### Membres:\n",
        "\n",
        "- Hugo Cadet\n",
        "- Henri Ngo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjL967e1Ek7a"
      },
      "source": [
        "## Description:\n",
        "\n",
        "Ce projet est dédié à l'utilisation des n-grammes en traitement du langage naturel (NLP). Les n-grammes sont des séquences de n mots consécutifs dans un texte, et ils sont présent dans de nombreuses applications du NLP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRu928r0Ek7b"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1.  Chargement et pré-traitement des données\n",
        "\n",
        "<a name='1.1'></a>\n",
        "### 1.1 Chargement des données\n",
        "\n",
        "Les données qu'on a utilisé dans ce travail sont contenues dans le ichier [trump.txt](./trump.txt) <br>\n",
        "Cf ce qui suit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "K_zgp803Ek7b",
        "outputId": "39175437-6dc5-4990-a7e2-48e66bd53eaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thank you very much.\\nWe had an amazing convention.\\nThat was one of the best.\\nI think it was one of the best ever.\\nIn terms -- in terms of enthusiasm, in terms of I think what it represents, getting our word out.\\nIvanka was incredible last night.\\nShe did an incredible job.\\nAnd so many of the speakers'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "filename = \"trump.txt\"\n",
        "\n",
        "with open(filename, 'r') as fichier:\n",
        "    # Lire tout le contenu du fichier en une seule fois\n",
        "    data = fichier.read()\n",
        "\n",
        "data[:300]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvx-hsxZEk7b"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2  Segmentation\n",
        "\n",
        "Pré-traitement des données en suivant les étapes suivantes:\n",
        "\n",
        "1. Enlever les majuscules.\n",
        "2. Remplacer les \"\\n\" par des espaces\n",
        "3. Séparer les données en phrases en utilisant les délimiteurs suivants `.`, `?` et `!` comme séparateur.\n",
        "4. Enlever les signes de ponctuation (Attention de garder les espaces).\n",
        "5. Enlever les phrases vides.\n",
        "6. Segmenter les phrases avec la fonction nltk.word_tokenize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkhTLwpzEk7c",
        "outputId": "7663a804-8740-420b-8f4c-4c5a7e20095b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from os import replace\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def preprocess(data):\n",
        "    data = data.lower()\n",
        "    data = data.replace('\\n', ' ')\n",
        "    sentences = re.split(r'!|\\.|\\?', data)\n",
        "    sentences = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in sentences]\n",
        "    sentences = [sentence for sentence in sentences if sentence.strip()]\n",
        "    tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "data = preprocess(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "x = \"Cats are independent.\\nDogs are faithful.\"\n",
        "preprocess(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnOWOJnlk1D3",
        "outputId": "c98cf287-3ce6-48fa-c0b3-14d00272a631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlnfXKiGEk7c"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeRUIuTiEk7c"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "###  1.3 Création d'ensembles d'entraînement et de test\n",
        "\n",
        " **aléatoirement** 80% des données pour l'ensemble d'entrainement.\n",
        " <br> 20% pour l'ensemble de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2DMmT4DEk7c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test = train_test_split(data, test_size=0.20, train_size=0.80, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtKSOS8fdr0P",
        "outputId": "57531218-040d-4929-c42f-b8828e073982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'a', 'combination', 'those', 'two', 'are', 'right']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC4Xym74Ek7c"
      },
      "source": [
        "<a name='1.4'></a>\n",
        "### 1.4 Construction du vocabulaire\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn6D2gVoEk7c"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_voc(documents, threshold): #document : List ? , document : train_set ? ok for now\n",
        "  jetons=[]\n",
        "  cnt = Counter()\n",
        "  for document in documents:\n",
        "    for mot in document:\n",
        "      cnt[mot] +=1\n",
        "      if cnt[mot] >= threshold and mot not in jetons :\n",
        "        jetons.append(mot)\n",
        "  return jetons\n",
        "\n",
        "voc=build_voc(X_train,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiPwUQGhEk7c"
      },
      "source": [
        "<a name='1.5'></a>\n",
        "### 1.5 Mots hors vocabulaire\n",
        "\n",
        "\n",
        "Gestion des mots hors vocabulaire (Out of Vocabulary) <b>OOV</b>.\n",
        "Le pourcentage de mots inconnus dans l'ensemble de test est appelé le taux de mots <b> OOV </b>.\n",
        "\n",
        "Pour gérer les mots inconnus lors de la prédiction, utilisez un jeton spécial 'unk' pour représenter tous les mots inconnus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFPfS0zxEk7d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def replace_oov(tokenized_sentences, voc):\n",
        "    token_sentences=[]\n",
        "    for sentence in tokenized_sentences:\n",
        "      Sentence = []\n",
        "      for mot in sentence:\n",
        "        if mot in voc :\n",
        "          Sentence.append(mot)\n",
        "        else:\n",
        "          mot = '<unk>'\n",
        "          Sentence.append(mot)\n",
        "      token_sentences.append(Sentence)\n",
        "    return token_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTeZJMycEk7d",
        "outputId": "d592714b-597b-4538-8ef9-5054b2141bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase initiale:\n",
            "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
            "Phrase segmentée avec'<unk>':\n",
            "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]]\n",
        "vocabulary = build_voc([[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]], 2)\n",
        "tmp_replaced_tokenized_sentences = replace_oov(tokenized_sentences, vocabulary)\n",
        "print(f\"Phrase initiale:\")\n",
        "print(tokenized_sentences)\n",
        "print(f\"Phrase segmentée avec'<unk>':\")\n",
        "print(tmp_replaced_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBWsCDjrEk7d"
      },
      "source": [
        "##### Sortie attendue\n",
        "```CPP\n",
        "Phrase initiale:\n",
        "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
        "Phrase segmentée avec '<unk>':\n",
        "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3KycmK0Ek7d"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2. Modèles de langue n-gramme\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOdJ4AAZEk7d"
      },
      "source": [
        "Dans cette section, on developpe un modèle de langue n-grammes. Nous allons utiliser la formule:\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
        "\n",
        "- La fonction $C(\\cdots)$ représente le nombre d'occurrences de la séquence donnée.\n",
        "- $\\hat{P}$ signifie l'estimation de $P$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaK0686mEk7d"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 Fréquence des n-grammes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JpREjCtEk7d"
      },
      "source": [
        "\n",
        "\n",
        "On commence par mettre en œuvre une fonction qui calcule la fréquence des n-grammes pour un nombre arbitraire $n$.\n",
        "\n",
        "En pré-traitant la phrase en ajoutant $n$ marqueurs de début de phrase \"\\<s\\>\" pour indiquer le commencement de la phrase.\n",
        "\n",
        "- Par exemple, dans un modèle bigramme (N=2), la séquence devrait commencer avec deux jetons de début \"\\<s\\>\\<s\\>\". Ainsi, si la phrase est \"J'aime la nourriture\", modifiez-la pour devenir \"\\<s\\>\\<s\\> J'aime la nourriture\".\n",
        "- Ajoutez aussi un jeton de fin \"\\<e\\>\" pour que le modèle puisse prédire quand terminer une phrase.\n",
        "    \n",
        "    \n",
        "Ensuite, chaque n-gram est stocké en dictionnaire où :\n",
        "- La clé de chaque paire clé-valeur dans le dictionnaire est un tuple de n mots (et non une liste).\n",
        "- La valeur dans la paire clé-valeur est le nombre d'occurrences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "115krtEYEk7d"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "    cnt = Counter()\n",
        "    for sentence in data:\n",
        "      L=[]\n",
        "      sentence = [start_token]*n + sentence + [end_token]\n",
        "      i=0\n",
        "      while i+n<= len(sentence):\n",
        "        ngram = tuple(sentence[i:i+n])\n",
        "        cnt[ngram]+=1\n",
        "        i+=1\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6DaIEKEk7d",
        "outputId": "f3ea9931-9e73-451f-ca06-a64443906c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigramme:\n",
            "Counter({('<s>',): 2, ('mouse',): 2, ('<e>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('this',): 1, ('likes',): 1, ('cats',): 1})\n",
            "Bigrammes:\n",
            "Counter({('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1})\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "print(\"unigramme:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bigrammes:\")\n",
        "print(count_n_grams(sentences, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbu9XW9nEk7d"
      },
      "source": [
        "Sortie attendue:\n",
        "\n",
        "```CPP\n",
        "Unigrammes:\n",
        "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
        "Bigrammes:\n",
        "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADCq7xZ4Ek7d"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 Estimé du maximum de vraisemblance MLE\n",
        "\n",
        "#### 2.2.1 Calcul de probabilité pour un mot\n",
        "\n",
        "\n",
        "Ensuite, estimation de la probabilité d'un mot étant donnés les 'n' mots précédents avec les fréquences obtenues.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2}$$\n",
        "\n",
        "\n",
        "La fonction prend en entrée:\n",
        "\n",
        "- word : le mot dont on veut estimer la probabilité\n",
        "- previous_n_gram : le n-gramme précédent, sous forme de tuple\n",
        "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
        "- n_plus1_gram_counts: dictio n-gramme précédent plus le mot actuel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1rlI-9WEk7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts):\n",
        "    frqce_n = n_gram_counts[previous_n_gram]\n",
        "    frqce_plus1 = n_plus1_gram_counts[previous_n_gram+[word]]\n",
        "\n",
        "    if frqce_plus1 > 0:\n",
        "        return frqce_plus1/frqce_n\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDr5Yc0iEk7e"
      },
      "source": [
        "Nous ne verrons jamais assez de données pour estimer ces\n",
        "probabilités avec précision. Supposons que le n-gramme (\"le\", \"petit\") n'apparaît jamais dans les données d'entraînement. Si vous appelez estimate_probability(\"chat\", (\"le\", \"petit\"), n_gram_counts, n_plus1_gram_counts), la fonction renverra une probabilité de 0, ce qui n'est pas réaliste. De plus, la fonction aurait une exeption d'erreur si jamais elle trouvera une fréquence nulle en dénominateur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKSPeiNBEk7e"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3  Lissage add-k\n",
        "\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyOBSBT_Ek7e"
      },
      "outputs": [],
      "source": [
        "def estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    frqce_n = n_gram_counts[tuple(previous_n_gram)]\n",
        "    frqce_plus1 = n_plus1_gram_counts[tuple(previous_n_gram+[word])]\n",
        "    return (frqce_plus1+ k)/(frqce_n +k*vocabulary_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxQAE4fUEk7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f95d1ea-461f-4d10-eebe-0955a2b6936c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "tmp_prob = estimate_probability_smoothing(\"have\", ['<s>', 'i'], bigram_counts, trigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\" La probabilité de 'have' étant donné le mot précédent 'i' est: {tmp_prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txn6HEAPEk7e"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 Calcul des probabilités des n-grammes\n",
        "\n",
        "#### 2.4.1. Estimation des probabilités\n",
        "Création de la fonction estimate_probabilities qui calcule pour chaque mot du vocabulaire la probabilité d'être généré en utilisant la fonction avec lissage add-k.\n",
        "\n",
        "\n",
        "Cette fonction prends en entrée:\n",
        "- previous_n_gram\n",
        "- n_gram_counts\n",
        "- n_plus1_gram_counts\n",
        "- vocabulary: le vocabulaire\n",
        "- k: la constante de lissage\n",
        "\n",
        "La fonction retourne un dictionnaire ayant pour clés tous les mots du vocabulaire ainsi que leur probabilité d'être générés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZNf2lofEk7e"
      },
      "outputs": [],
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "    dico = {}\n",
        "    vocabulary.append('<e>')\n",
        "    for word in vocabulary: #add <e>\n",
        "      frqce = estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, len(vocabulary), k)\n",
        "      dico[word]= frqce\n",
        "    return dico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blo0sdzAEk7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0fa8d1-9d4b-4f6a-ca66-2426d4496b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cats': 0.1111111111111111,\n",
              " 'mouse': 0.2222222222222222,\n",
              " 'this': 0.1111111111111111,\n",
              " 'a': 0.1111111111111111,\n",
              " 'have': 0.1111111111111111,\n",
              " 'i': 0.1111111111111111,\n",
              " 'likes': 0.1111111111111111,\n",
              " '<e>': 0.1111111111111111}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "estimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meaZEd63Ek7e"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "{'likes': 0.1111111111111111,\n",
        " 'have': 0.1111111111111111,\n",
        " 'this': 0.1111111111111111,\n",
        " 'i': 0.1111111111111111,\n",
        " 'mouse': 0.2222222222222222,\n",
        " 'a': 0.1111111111111111,\n",
        " 'cats': 0.1111111111111111,\n",
        " '<e>': 0.1111111111111111}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhaLpzQ4Ek7e"
      },
      "source": [
        "#### 2.4.2. Probabilités étant donné un contexte\n",
        "\n",
        "On affiche maintenant les probabilités des tri-grammes étant donné le context \"i will\" en utilisant les données d'entraînement, .limit(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzZdnPNEEk7e"
      },
      "outputs": [],
      "source": [
        "bigram_counts = count_n_grams(X_train, 2)\n",
        "trigram_counts = count_n_grams(X_train, 3)\n",
        "\n",
        "x = sorted(estimate_probabilities([\"i\",\"will\"], bigram_counts, trigram_counts, voc, k=1).items(),key=lambda x:x[1],reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzdFSAKilFIg",
        "outputId": "24b2658f-4fed-43fa-9afe-a66b8ea1bfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tell', 0.0070981916511745815), ('fix', 0.005239141456819334), ('fight', 0.005239141456819334), ('be', 0.0050701368936961295), ('never', 0.004225114078080108), ('say', 0.003718100388710495), ('not', 0.0025350684468480648), ('ask', 0.0023660638837248605), ('also', 0.0018590501943552475), ('work', 0.001521041068108839)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkSPYqT3Ek7e"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "[('tell', 0.0070981916511745815),\n",
        " ('fix', 0.005239141456819334),\n",
        " ('fight', 0.005239141456819334),\n",
        " ('be', 0.0050701368936961295),\n",
        " ('never', 0.004225114078080108),\n",
        " ('say', 0.003718100388710495),\n",
        " ('not', 0.0025350684468480648),\n",
        " ('ask', 0.0023660638837248605),\n",
        " ('also', 0.0018590501943552475),\n",
        " ('work', 0.001521041068108839)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP-gpRiUEk7e"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3. Perplexité\n",
        "\n",
        "Dans cette section, on génère le score de perplexité pour évaluer votre modèle sur l'ensemble de test. **A Approfondir**\n",
        "\n",
        "Pour calculer le score de perplexité d'une phrase sur un modèle n-gramme :\n",
        "\n",
        "$$PP(W) =\\sqrt[N]{ \\prod_{t=1}^{N} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}$$\n",
        "\n",
        "où N = le nombre de jeton dans la phrases incluant le jeton \\<e\\>\n",
        "et P = la probabilité de générer le jeton $w_t$\n",
        "\n",
        "Plus les probabilités sont élevées, plus la perplexité sera basse.\n",
        "\n",
        "<a name='3.1'></a>\n",
        "### 3.1. Calcul de la perplexité\n",
        "On créer la fonction `calculate_perplexity`, qui pour une phrase donnée, nous donne le score de perplexité."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "    # Set n to the order of the n-gram model (for example, 1 for unigram, 2 for bigram, etc.)\n",
        "    counter_items = list(n_gram_counts.items())\n",
        "    n = len(counter_items[0][0])\n",
        "\n",
        "    # Add tokens \"<s>\" \"<e>\" to sentence\n",
        "    words = ['<s>']*n + sentence + ['<e>']\n",
        "\n",
        "    # initialize perplexity\n",
        "    perplexity = 1.0\n",
        "\n",
        "    # Calculate the number of words in the sentence\n",
        "    num_words = len(words)\n",
        "\n",
        "    for i in range(n - 1, num_words-1):\n",
        "        # Extract the previous n-gram (context) and the target word\n",
        "        previous_n_gram = words[i - (n - 1) : i+1]\n",
        "        target_word = words[i+1]\n",
        "\n",
        "        # Calculate the conditional probability using n-gram and (n+1)-gram counts\n",
        "        prob = estimate_probability_smoothing(target_word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0)\n",
        "\n",
        "        # Update perplexity\n",
        "        perplexity *= 1.0 / prob\n",
        "\n",
        "    perplexity_finale = math.pow(perplexity, 1/(num_words-n))\n",
        "\n",
        "    return perplexity_finale"
      ],
      "metadata": {
        "id": "VsWc_YXiE3aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2T8RtEZEk7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa659f53-afff-4a00-e8bc-b910c77414c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexité de la première phrase : 4.1930\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "\n",
        "perplexity = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "print(f\"Perplexité de la première phrase : {perplexity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xetd-niEk7f"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2. Perplexité sur une phrase d'entraînement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70b35aa-8e6e-4dd0-c7f9-141566609577",
        "id": "gOZRqXC5h8aO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexité de la première phrase : 31.4048\n"
          ]
        }
      ],
      "source": [
        "# test de modèle bi-grammes :\n",
        "\n",
        "voc_1 = build_voc(X_train,1000)\n",
        "\n",
        "bigram_counts = count_n_grams(X_train, 2)\n",
        "trigram_counts = count_n_grams(X_train, 3)\n",
        "\n",
        "\n",
        "perplexity = calculate_perplexity(X_train[0], bigram_counts, trigram_counts, len(voc_1), k=0.01)\n",
        "print(f\"Perplexité de la première phrase : {perplexity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test de modèle tri-grammes :\n",
        "\n",
        "trigram_counts = count_n_grams(X_train, 3)\n",
        "quadrigram_counts = count_n_grams(X_train, 4)\n",
        "\n",
        "\n",
        "perplexity = calculate_perplexity(X_train[0],\n",
        "                                         trigram_counts, quadrigram_counts,\n",
        "                                         len(voc_1), k=0.01)\n",
        "print(f\"Perplexité de la première phrase : {perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SldRsufzYdCh",
        "outputId": "87671990-c331-4e03-eeb9-8f4a7387e267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexité de la première phrase : 30.2854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test de modèle quadri-grammes :\n",
        "\n",
        "quadrigram_counts = count_n_grams(X_train, 4)\n",
        "quintigram_counts = count_n_grams(X_train, 5)\n",
        "\n",
        "\n",
        "perplexity = calculate_perplexity(X_train[0],\n",
        "                                         trigram_counts, quadrigram_counts,\n",
        "                                         len(voc_1), k=0.01)\n",
        "print(f\"Perplexité de la première phrase : {perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DqX6p23iKt5",
        "outputId": "9b5d0acc-8fed-40a8-c977-f80285c28f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexité de la première phrase : 30.2854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PPVL19UEk7f"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3. Perplexité du corpus de test\n",
        "\n",
        "#### 3.3.1. On peut maintenant calculer et afficher la perplexité des modèles bi-grammes, tri-grammes et quadri-grammes sur votre corpus de test. K=1 ici.\n",
        "\n",
        "Pour calculer la perplexité d'un corpus de *m* phrases, il suffit de suivre la formule suivante :\n",
        "\n",
        "Soit $N$ le nombre total de jetons dans le corpus de test C et $N_i$ le nombre de jetons dans la phrase i.\n",
        "\n",
        "$$Perplexity(C) = \\Big(\\frac{1}{P(s_1, ..., s_m)}\\Big)^{1/N}$$\n",
        "$$P(s_1, ..., s_m) = \\prod_{i=1}^{m} p(s_i)$$\n",
        "$$p(s_i) = \\prod_{t=1}^{N_i} \\hat{P}(w_t | w_{t-n} \\cdots w_{t-1})$$\n",
        "\n",
        "Puisqu'il s'agit d'un multiplication de probabilités (situées entre 0 et 1), le produit devient nul très rapidement. C'est pourquoi il est plus efficace d'effectuer une transformation vers un espace logarithmique pour transformer les multiplications en addition. Cela donne ainsi la formule suivante:\n",
        "\n",
        "$$LogPerplexity(C) = 2^{-\\frac{1}{N} \\sum_{k=1}^{m} log_{2} \\; p(s_k)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mni39N-gPvcO"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity_corpus(corpus, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "  perplexity_sentences = 0\n",
        "  T = 0\n",
        "\n",
        "  for phrase in corpus :\n",
        "    T += len(phrase)+1\n",
        "    perplexity_sentences += math.log2(calculate_perplexity(phrase, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k))\n",
        "\n",
        "  Log_Perplexity_corpus = math.pow(2, (-1)*perplexity_sentences/T)\n",
        "\n",
        "  return (Log_Perplexity_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1fc903-f735-408d-d12a-bc8fd9e85c87",
        "id": "kspaJbVRPz5I"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5170269930330363"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ],
      "source": [
        "# n_gram_counts = {('<s>', 'quick'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
        "# n_plus1_gram_counts = { ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
        "\n",
        "train_corpus = [[\"the\", \"quick\", \"brown\", \"fox\"], [\"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]]\n",
        "n_gram_counts = {('<s>', '<s>'): 2, ('<s>', 'the'): 1, ('<s>', 'jumps'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', '<e>'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
        "n_plus1_gram_counts = {('<s>', '<s>', '<s>', ): 2, ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1,  ('<s>', '<s>', 'jumps', ): 1, ('<s>', 'jumps', 'over'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('brown', 'fox', '<e>'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
        "\n",
        "vocabulary = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"<e>\"]\n",
        "\n",
        "N = 5\n",
        "V = len(vocabulary)\n",
        "\n",
        "test_corpus = [[\"the\", \"fox\"], [\"jumps\"]]\n",
        "\n",
        "\n",
        "# Complétez le calcul de la perplexité avec k=1\n",
        "\n",
        "# On a pris les 2 derniers gram_counts\n",
        "n_gram_counts = Counter(n_gram_counts)\n",
        "n_plus1_gram_counts = Counter(n_plus1_gram_counts)\n",
        "\n",
        "calculate_perplexity_corpus(test_corpus, bigram_counts, trigram_counts, V, 1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o53e8RxXEk7f"
      },
      "source": [
        "#### Sortie attendue\n",
        "\n",
        "    Perplexité du corpus de test:  7.708920690856638"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01424d1-7980-4b02-dde9-9d92990fe923",
        "id": "CfeScjcQQC1x"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6411413282064495"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ],
      "source": [
        "# Calculez mainenant la perplexité de votre corpus de test\n",
        "\n",
        "voc_2 = build_voc(X_test,2)\n",
        "\n",
        "bigram_counts = count_n_grams(X_test, 2)\n",
        "trigram_counts = count_n_grams(X_test, 3)\n",
        "\n",
        "calculate_perplexity_corpus(X_test, bigram_counts, trigram_counts, len(voc_2), 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PE_zCvIEk7f"
      },
      "source": [
        "> On attends d'avoir des perplexités plus élevées que celles obtenues sur l'ensemble d'entraînement, vu que généralement les modèles performent bien en phase d'entraînement qu'en phase de test. D'autre part, on pourrait expliquer ces résultats obtenus du fait que le corpus de test est plus simple, la taille du corpus de test est aussi tellement petit que celle d'entraînement ce qui facilite d'obtenir des performances en test beaucoup mieux qu'en entraînement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQYvxZCGEk7f"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4. Construction d'un modèle d'auto-complétion\n",
        "\n",
        "Dans cette dernière partie, on utilise enfin les modèles n-grammes construits aux numéros précédents afin de faire un modèle d'autocomplétion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN2cF9x-Ek7f"
      },
      "source": [
        "<a name='4.1'></a>\n",
        "### 4.1 Suggestion d'un mot à partir d'un préfixe\n",
        "\n",
        "\n",
        "La première étape sera de construire une fonction qui suggère un mot à partir des premiers caractères entrés par un utilisateur, considérant un seul type de n-gramme.  \n",
        "\n",
        "La fonction `suggest_word` retourne le mot le plus probable avec la probabilité associée"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyfEcWL9Ek7g"
      },
      "outputs": [],
      "source": [
        "def suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, prefixe=\"\"):\n",
        "      dico =estimate_probabilities(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k) #pb proba uniforme\n",
        "      keys = list(dico.keys())\n",
        "      idx = None\n",
        "      for key in keys:\n",
        "        if key.startswith(prefixe):\n",
        "          if idx == None :\n",
        "            idx = key\n",
        "          elif dico[idx] < dico[key]:\n",
        "            idx = key\n",
        "      if idx == None : return \"error\",0\n",
        "      else : return idx, dico[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIn7L3hsEk7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c21983-a287-457d-8ff4-b444b3fd8123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " avec les mots précédents 'i have',\n",
            "\t le mot suggéré est `a` avec la probabilité 0.2000\n",
            "\n",
            "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
            "\t le mot suggéré est : `mouse` avec une probabilité de 0.1000\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "\n",
        "previous_tokens = [\"i\", \"have\"]\n",
        "# tmp_suggest1= suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "\n",
        "tmp_suggest1_1 = suggest_word(previous_tokens, bigram_counts, trigram_counts, unique_words, k=1.0)\n",
        "\n",
        "print(f\" avec les mots précédents 'i have',\\n\\t le mot suggéré est `{tmp_suggest1_1[0]}` avec la probabilité {tmp_suggest1_1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "tmp_starts_with = 'm'\n",
        "tmp_suggest2 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, prefixe=tmp_starts_with)\n",
        "print(f\"avec les mots précédents 'i have', et une suggestion qui commence par `{tmp_starts_with}`\\n\\t le mot suggéré est : `{tmp_suggest2[0]}` avec une probabilité de {tmp_suggest2[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAIfA3dYEk7g"
      },
      "source": [
        "### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "avec les mots précédents 'i have',\n",
        "\t le mot suggéré est `a` avec la probabilité 0.2222\n",
        "\n",
        "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
        "\t le mot suggéré est : `mouse` avec une probabilité de 0.1111\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec9TW-3uEk7g"
      },
      "source": [
        "<a name='4.2'></a>\n",
        "### 4.2 Suggestions multiples\n",
        "\n",
        "Afin de suggérer plusieurs mots à l'utilisateur, une stratégie que l'on peut utiliser est de retourner un ensemble de mots suggérés par plusieurs types de modèles n-grammes.\n",
        "\n",
        "En utilisant la fonction `suggest_word` du numéro précédent, on complete la fonction `get_suggestions` qui retourne les suggestions des modèles n-grammes passés en paramètre.\n",
        "\n",
        "La fonction get_suggestions prends en paramètres:\n",
        "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
        "- n_gram_counts_list: une liste de n-grammes dans l'ordre suivant [unigrammes, bigrammes, trigrammes, quadrigrammes, ...]\n",
        "- vocabulary_size: la taille du vocabulaire\n",
        "- k: la constante de lissage (entre 0 et 1)\n",
        "- prefixe: Le début du mot que l'on veut prédire, \"\" si au aucun préfixe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "331wmXuiOGZb"
      },
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, prefixe=\"\"):\n",
        "\n",
        "  L=[]\n",
        "  tmp_suggest_multip = []\n",
        "  for i in range(len(n_gram_counts_list)-1):\n",
        "    tmp_suggest_multip.append(suggest_word(previous_tokens, n_gram_counts_list[i], n_gram_counts_list[i+1], vocabulary, k=1.0, prefixe=\"\"))\n",
        "\n",
        "  # trier en ordre décroissant\n",
        "  tmp_suggest_multip_sorted = sorted(tmp_suggest_multip, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  return list(set([tmp_suggest_multip_sorted[0][0], tmp_suggest_multip_sorted[1][0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "9a35f4ec-0a63-40da-a4ee-7205d2bcdff0",
        "id": "jQpV0xSZOSXr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cats': 0.125, 'i': 0.125, 'have': 0.125, 'mouse': 0.125, 'likes': 0.125, 'this': 0.125, 'a': 0.125, '<e>': 0.125}\n",
            "{'cats': 0.1, 'i': 0.1, 'have': 0.1, 'mouse': 0.1, 'likes': 0.1, 'this': 0.1, 'a': 0.2, '<e>': 0.1}\n",
            "{'cats': 0.1, 'i': 0.1, 'have': 0.1, 'mouse': 0.1, 'likes': 0.1, 'this': 0.1, 'a': 0.1, '<e>': 0.1}\n",
            "{'cats': 0.09090909090909091, 'i': 0.09090909090909091, 'have': 0.09090909090909091, 'mouse': 0.09090909090909091, 'likes': 0.09090909090909091, 'this': 0.09090909090909091, 'a': 0.09090909090909091, '<e>': 0.09090909090909091}\n",
            "Etant donné les mots i have, je suggère :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['cats', 'a']"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "quadgram_counts = count_n_grams(sentences, 4)\n",
        "qintgram_counts = count_n_grams(sentences, 5)\n",
        "\n",
        "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
        "previous_tokens = [\"i\", \"have\"]\n",
        "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
        "\n",
        "print(f\"Etant donné les mots i have, je suggère :\")\n",
        "display(tmp_suggest3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sortie obtenue : Etant donné les mots i have, je suggère :\n",
        "['cats', 'a']"
      ],
      "metadata": {
        "id": "TPN4tzaRQdrF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1acFQ9NEk7g"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "Etant donné les mots i have, je suggère :\n",
        "['a']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pP7QqsPEk7g"
      },
      "source": [
        "<a name='4.3'></a>\n",
        "### 4.3 Autocomplétion\n",
        "\n",
        "Il est maintenant temps de combiner nos fonctions afin de créer le modèle d'autocomplétion. En utilisant le jeu de données d'entraînement, on calcule la fréquence des n-grammes allant de 1 à 5 et utilisez la fonction *get_suggestions* afin de suggérer des mots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtY5mWFxEk7g"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "text_input = widgets.Text(placeholder=\"Entrez votre text ici...\")\n",
        "\n",
        "suggestions_label = widgets.Label(v Autocomplétion et génération de phrases avec des modèles de langue n-grammesalue=\"Suggestions: \")\n",
        "\n",
        "def update_suggestions(change):\n",
        "    texte_actuel = change[\"new\"]\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    top_suggestions = [\"word1\", \"word2\"]\n",
        "    suggestions_label.value = \"Suggestions: \" + \", \".join(top_suggestions)\n",
        "\n",
        "text_input.observe(update_suggestions, names=\"value\")\n",
        "\n",
        "display(text_input)\n",
        "display(suggestions_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRefCIhwEk7g"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Modèle de génération de phrases\n",
        "\n",
        "Dans cette partie on construit un modèle de génération de phrases en utilisant les n-grammes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La stratégie de modèle N-gramme, faite en 2, a un contexte fixe et limité dépendant de la taille des N-grammes. Elle ne tiennent pas compte du contexte global ou d'informations non locales, ce qui la rend moins efficaces pour comprendre la signification complète d'un mot ou d'une phrase. Du coup, elle sera moins capables de généraliser efficacement sur des mots ou des séquences de mots qui ne se sont pas produits exactement dans les données d'entraînement."
      ],
      "metadata": {
        "id": "wP4BeiPAGedW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_CO2HGLEk7h"
      },
      "source": [
        "<a name='5.1'></a>\n",
        "\n",
        "### 5.1 Génération stochastique de mots\n",
        "\n",
        "On recode la fonction suggest_word afin d'utiliser une suggestion stochastique. Autrement dit, au lieu de retourner le mot le plus probable, la fonction génére le mot suivant selon sa probabilité.\n",
        "\n",
        "Par exemple si le mot 'like' a la probabilité 0.25 d'être généré, alors il sera retourné 25% du temps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC3D6h998Iu_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def suggest_word_with_probs(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "\n",
        "    # Estimez les probabilités des mots candidats\n",
        "    probabilities = estimate_probabilities(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k)\n",
        "\n",
        "    # le 2ème paramètre k = 1  est pour choisir enfin un seul mot :\n",
        "    return random.choices(list(probabilities.keys()), list(probabilities.values()), k = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLUK4iz4Ek7h"
      },
      "source": [
        "<a name='5.2'></a>\n",
        "### 5.2 Générations de phrases\n",
        "\n",
        "#### 5.2.1. Génération stochastique\n",
        "Finalement, on complète maintenant la fonction `generate_sentence` qui génère une phrase longue de n_words en appelant votre nouvelle fonction `suggest_words_with_probs`. La génération doit s'arrêter si le modèle génère un jeton de fin de phrase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCmjC_h0f1im"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.0001):\n",
        "\n",
        "  # Set n to the order of the n-gram model (for example, 1 for unigram, 2 for bigram, etc.)\n",
        "  n = len(next(iter(n_gram_counts)))\n",
        "  S = ['<s>']*n\n",
        "  P = S\n",
        "  while(len(S)<n_words):\n",
        "    w = suggest_word_with_probs(P, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0)\n",
        "    S = S + w\n",
        "    if w == ['<e>']:\n",
        "      return S\n",
        "    else:\n",
        "      P = S[len(S)-n:len(S)]\n",
        "  return S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZb24NYgEk7h"
      },
      "source": [
        "#### 5.2.2. Test sur des n-grammes\n",
        "On teste ensuite votre fonction avec des trigrammes et des 5-grammes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e2fdd7-c70e-47fb-966a-0888e0ead14b",
        "id": "1B84vq01f6x5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '<s>', 'likes', 'have', 'my', 'have']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "sentences_exp = [['i', 'have', 'a', 'mouse', 'in', 'my','house'],\n",
        "             ['this', 'mouse', 'likes', 'cats', 'and', 'dogs']]\n",
        "unique_words_exp = list(set(sentences_exp[0] + sentences_exp[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences_exp, 1)\n",
        "bigram_counts = count_n_grams(sentences_exp, 2)\n",
        "trigram_counts = count_n_grams(sentences_exp, 3)\n",
        "quadrigram_counts = count_n_grams(sentences_exp, 4)\n",
        "quintigram_counts = count_n_grams(sentences_exp, 5)\n",
        "\n",
        "generate_sentence(6, bigram_counts, trigram_counts, unique_words_exp, k=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea3ec8d7-d322-43e0-823e-d5fdc358a114",
        "id": "SNiJqvY8gWdB"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '<s>', '<s>', '<s>', 'a', 'mouse', 'likes', 'my', 'my', 'house']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "generate_sentence(10, quadrigram_counts, quintigram_counts, unique_words_exp, k=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81yM-EJFpMAl"
      },
      "source": [
        ">On remarque que la génération du mot le plus fréquent a diminué, dans notre cas \"mouse\". Théoriquement si le lissage est excessif, la probabilité de chaque N-gramme tend à être plus uniforme. Cela signifie que les N-grammes rares deviennent plus probables et les N-grammes fréquents deviennent moins probables. Dans le but d'améliorer la situation, on peut envisager à priori un réglage du paramètre k : On essaie différentes valeurs de k jusqu'à avoir de meilleurs résultats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9KlusWoEk7h"
      },
      "source": [
        "#### 5.2.4.  Question sur la valeur de la constante kn si elle est trop petite :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1YJ9Xiku_8j"
      },
      "source": [
        ">Les problèmes envisageable en ce cas sont :\n",
        "\n",
        "*   Possibilité d'avoir des probabilités nulles\n",
        "*   Les N-grammes peu fréquents, observés seulement une ou quelques fois, afficheront des probabilités très faibles, ce qui peut conduire à des erreurs significatives lorsque ces séquences apparaissent dans le texte.\n",
        "*   Une perplexité élevée, ce qui se traduit par la difficulté du modèle à fournir des estimations précises de probabilité pour les phrases, car il assignera des probabilités très faibles à de nombreuses séquences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PzWHezGEk7h"
      },
      "source": [
        "<a name='5.3'></a>\n",
        "### 5.3. Amélioration de la génération stochastique de mots\n",
        "\n",
        "#### 5.3.1. Amélioration stochastique\n",
        "\n",
        "Comme on a pu l'observer, la génération stochastique, bien qu'elle soit efficace pour générer des phrases différentes, a tendance à ne pas générer des phrases toujours cohérentes. On propose aisi une amélioration de la méthode `suggest_word` avec la méthode `suggest_word_new` permettant de générer des phrases plus cohérentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-ShoOVlAwLn"
      },
      "source": [
        ">\n",
        "\n",
        "*   On choisira la bonne valeur de k suivant la meilleure performance. Premièrement, On a constaté expérimentalement dans la question 3 que dans notre contexte, Threshold = 1000 genère de bons résultats quand même.\n",
        "*   On va ajuster la taille du vocabulaire suivant la meilleure performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8BV9ToeEk7i"
      },
      "source": [
        "##### b) Implémentation de la méthode proposée"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voc_2 = build_voc(X_train,1000)\n",
        "\n",
        "bigram_counts = count_n_grams(X_train, 2)\n",
        "trigram_counts = count_n_grams(X_train, 3)\n",
        "quatrigram_counts = count_n_grams(X_train, 4)\n",
        "quintigram_counts = count_n_grams(X_train, 5)"
      ],
      "metadata": {
        "id": "tp7hDLZug1Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]\n",
        "# Perform cross-validation to find the best value of k\n",
        "best_k = None\n",
        "best_perplexity = float('inf')\n",
        "\n",
        "for k in k_values:\n",
        "    perplexity = calculate_perplexity_corpus(X_train, bigram_counts, trigram_counts, len(voc_2), k)\n",
        "    print(f'k = {k}: Perplexity = {perplexity}')\n",
        "\n",
        "    if perplexity < best_perplexity:\n",
        "        best_perplexity = perplexity\n",
        "        best_k = k\n",
        "print(f'Best k: {best_k}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whS5PNk30ZO1",
        "outputId": "b6cb903c-0449-4ea4-a092-ab934fe678b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k = 0.0001: Perplexity = 0.7687053173040973\n",
            "k = 0.001: Perplexity = 0.7687053173040973\n",
            "k = 0.01: Perplexity = 0.7687053173040973\n",
            "k = 0.1: Perplexity = 0.7687053173040973\n",
            "k = 0.5: Perplexity = 0.7687053173040973\n",
            "k = 1.0: Perplexity = 0.7687053173040973\n",
            "Best k: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vu que dans notre cas, tous les k performent identiquement ! On prend donc une valeur recommandée dans la littérature k = 0.1"
      ],
      "metadata": {
        "id": "0xnKr-Pu-RlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_values = [100, 500, 1000, 1500, 2000]\n",
        "# Perform cross-validation to find the best value of threshold\n",
        "best_threshold = None\n",
        "best_perplexity = float('inf')\n",
        "\n",
        "for threshold in threshold_values:\n",
        "  voc_2 = build_voc(X_train,threshold)\n",
        "  bigram_counts = count_n_grams(X_train, 2)\n",
        "  trigram_counts = count_n_grams(X_train, 3)\n",
        "\n",
        "  perplexity = calculate_perplexity_corpus(X_train, bigram_counts, trigram_counts, len(voc_2), k=0.1)\n",
        "  print(f'threshold = {threshold}: Perplexity = {perplexity}')\n",
        "\n",
        "  if perplexity < best_perplexity:\n",
        "        best_perplexity = perplexity\n",
        "        best_threshold = threshold\n",
        "print(f'Best threshold: {best_threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJf7CQjw61A0",
        "outputId": "19e9227c-57e6-4c86-c65c-d77e58abf90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "threshold = 100: Perplexity = 0.6911507979595707\n",
            "threshold = 500: Perplexity = 0.7444506894024\n",
            "threshold = 1000: Perplexity = 0.7687053173040973\n",
            "threshold = 1500: Perplexity = 0.785504586883338\n",
            "threshold = 2000: Perplexity = 0.7925368532942627\n",
            "Best threshold: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On trouve que la taille adéquate du vocabulaire est celle avec un seuil de répition minimal (Threshold) = 100"
      ],
      "metadata": {
        "id": "_YQ_ba6PEIgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reproduire le vocabulaire avec le nouveau seuil Threshold = 100 :\n",
        "voc_2 = build_voc(X_train,100)\n",
        "\n",
        "bigram_counts = count_n_grams(X_train, 2)\n",
        "trigram_counts = count_n_grams(X_train, 3)\n",
        "quatrigram_counts = count_n_grams(X_train, 4)\n",
        "quintigram_counts = count_n_grams(X_train, 5)"
      ],
      "metadata": {
        "id": "XBH1ML4PEpHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Sez2pSEk7i"
      },
      "outputs": [],
      "source": [
        "# paramètre k est changé à 0.1,\n",
        "# taille de vocabulaire a changé aussi pour un seuil de 100 :\n",
        "\n",
        "def suggest_word_new(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.1):\n",
        "  # Estimez les probabilités des mots candidats\n",
        "    probabilities = estimate_probabilities(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k)\n",
        "\n",
        "    return random.choices(list(probabilities.keys()), list(probabilities.values()), k = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0AHw-FOEk7i"
      },
      "source": [
        "#### 5.3.2. Génération améliorée\n",
        "Finalement :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nK76Wu9_iQF"
      },
      "outputs": [],
      "source": [
        "def generate_sentence_new(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.1):\n",
        "  # Set n to the order of the n-gram model (for example, 1 for unigram, 2 for bigram, etc.)\n",
        "  n = len(next(iter(n_gram_counts)))\n",
        "  S = ['<s>']*n\n",
        "  P = S\n",
        "  while(len(S)<n_words):\n",
        "    w = suggest_word_new(P, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0)\n",
        "    S = S + w\n",
        "    if w == ['<e>']:\n",
        "      return S\n",
        "    else:\n",
        "      P = S[len(S)-n:len(S)]\n",
        "  return S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY6If79LEk7i"
      },
      "source": [
        "#### 5.3.3. Test sur des n-grammes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vy1MTAsEk7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7efb2995-b15a-4590-a143-03804aa173bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '<s>', 'thank', 'you', '<e>']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "generate_sentence_new(8, bigram_counts, trigram_counts, voc_2, k=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCmAF8awEk7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d3ef7c-f97f-4f71-dbd7-2298fa64ca39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " 'we',\n",
              " 'need',\n",
              " 'law',\n",
              " 'theres',\n",
              " 'like',\n",
              " 'really',\n",
              " 'deal',\n",
              " 'number']"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "generate_sentence_new(12, quadrigram_counts, quintigram_counts, voc_2, k=0.1)"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC2-3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}