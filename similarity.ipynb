{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HugoKD/NLP/blob/main/similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal** : Given an anchor, a query, a positive and a negative give a higher similarity score to the positive than to the negative given the query and the anchor"
      ],
      "metadata": {
        "id": "FtiHU5PSsBGZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPN_B-Z3DRlQ"
      },
      "source": [
        "# TODO : Implémenter une version end-to-end différentiable\n",
        "## -> : inclure tous les calculs dans le graphe de calcul PyTorch pour permettre à la backpropagation\n",
        "##de remonter à travers toute la chaîne, y compris la génération T5 et l'encodage BERT.\n",
        "Aujourd'hui, seule la génération T5 est entraînée, et BERT est gelé (utilisé comme fonction de scoring externe).\n",
        "Dans cette version future, on envisagerait :\n",
        "   - soit d'intégrer l'encodage BERT au graphe pour éventuellement co-entraîner une fonction de scoring,\n",
        "   - soit de remplacer l'étape de génération (non différentiable) par un mécanisme differentiable (ex : sampling continu / softmax différentiable),\n",
        "   - soit d'utiliser une loss de type Reinforcement Learning (ex : REINFORCE ou reward-based learning) pour remonter la qualité d'une séquence générée,     même si elle passe par une étape non différentiable (comme decode + encode + score)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlJBcDcPuO6U"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qvBqTTOW27A"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer, losses\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import losses\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9Qwv6ExknD8"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pCu8P4la1XZ"
      },
      "outputs": [],
      "source": [
        "pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r_8IpdczCMp"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wKFWdqlvGPC"
      },
      "outputs": [],
      "source": [
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7tidpjtvIuN"
      },
      "outputs": [],
      "source": [
        "print(type(data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTKt7zxtvI_z"
      },
      "outputs": [],
      "source": [
        "print(data[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSL9Ey-B7NvX"
      },
      "outputs": [],
      "source": [
        "for i,t in enumerate(data) :\n",
        "  if t[\"anchor\"].lower().find('invention') == -1 :\n",
        "    if t[\"anchor\"].lower().find('TECHINCAL FIELD'.lower()) == -1:\n",
        "      if t[\"anchor\"].lower().find('TECHNICAL FIELD'.lower()) == -1:\n",
        "        print(i,t[\"anchor\"].lower().find('Disclosure'.lower()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWXzOm-hHomI"
      },
      "outputs": [],
      "source": [
        "for i, t in enumerate(data): #preference : invention, Technical Field, TECHINCAL FIELD, disclosure,This application\n",
        "  point = (t[\"anchor\"].lower().find('technical field'),t[\"anchor\"].lower().find('TECHINCAL FIELD'.lower()),t[\"anchor\"].lower().find('invention'))\n",
        "  if point == (-1,-1,-1):\n",
        "    print(i,t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJbOaObR1s9"
      },
      "source": [
        "## Prepro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1GIqHD-LBUc"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlh__Qc4LYk9"
      },
      "outputs": [],
      "source": [
        "def truncate(row):\n",
        "    row_lower = row.lower()\n",
        "    point = next((p for p in (\n",
        "        row_lower.find('technical field'),\n",
        "        row_lower.find('invention'),\n",
        "        row_lower.find('disclosure'),\n",
        "        row_lower.find('this application')\n",
        "    ) if p != -1), None)\n",
        "\n",
        "    if point is not None:\n",
        "        return row[point:]\n",
        "    else:\n",
        "        print(\"No match found in:\", row)\n",
        "        return None\n",
        "\n",
        "data['prepro_anchor'] = data['anchor'].apply(truncate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEfyXNE2NX-W"
      },
      "outputs": [],
      "source": [
        "l_prepro,l  = data['prepro_anchor'].apply(lambda x : len(x)),data['anchor'].apply(lambda x : len(x))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(l_prepro, bins=50, color= 'red')\n",
        "plt.hist(l, bins=50, color = 'blue')\n",
        "\n",
        "plt.xlabel(\"Nombre de mots\")\n",
        "plt.ylabel(\"Nombre de documents\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywNcZYKzTnh-"
      },
      "outputs": [],
      "source": [
        "l_prepro,l  = data['positive'].apply(lambda x : len(x)),data['negative'].apply(lambda x : len(x))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(l_prepro, bins=50, color= 'green')\n",
        "plt.hist(l, bins=50, color = 'purple')\n",
        "\n",
        "plt.xlabel(\"Nombre de mots\")\n",
        "plt.ylabel(\"Nombre de documents\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI5FTC5_OCq3"
      },
      "source": [
        "## Test avec la concaténation :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i1ru_fdLH-c"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSZy7gQmLbeo"
      },
      "outputs": [],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "bert = bert.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf_ujhODLd34"
      },
      "outputs": [],
      "source": [
        "#generate a response of our query given the anchor\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "T5 = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJfuGXLmGdkt"
      },
      "source": [
        "### Some other lil tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVjrbF_ZLdGd"
      },
      "outputs": [],
      "source": [
        "def get_bert_embedding(text,bert):\n",
        "\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    bert.to(device)\n",
        "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = bert(**tokens)\n",
        "\n",
        "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return sentence_embedding.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiVYXz_uNmfx"
      },
      "outputs": [],
      "source": [
        "def generate_T5(row):\n",
        "  inputs = tokenizer(row, return_tensors=\"pt\", padding=True,max_length=512).to(device)\n",
        "  output = T5.generate(\n",
        "      **inputs,\n",
        "      num_beams=4,\n",
        "      max_length = 512,\n",
        "      early_stopping=True\n",
        "  )\n",
        "  response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  print(row)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPaz17ws_xRq"
      },
      "outputs": [],
      "source": [
        "data = data[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u9kooqlG8ej"
      },
      "outputs": [],
      "source": [
        "data['inputs'] = \"Question : \" + data['query'] + 'Context : ' + data['prepro_anchor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPWgKsHvFRHO"
      },
      "outputs": [],
      "source": [
        "generate_T5(data.iloc[0]['inputs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_kfvwmGFvOU"
      },
      "outputs": [],
      "source": [
        "data.iloc[0]['query']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_8qrf7pInk_"
      },
      "outputs": [],
      "source": [
        "data['respose'] = data['inputs'].apply(generate_T5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVQntV9WBmqp"
      },
      "outputs": [],
      "source": [
        "data.rename(columns={'respose':'response'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdUhUnJJJX7k"
      },
      "outputs": [],
      "source": [
        "data['response_embedding'] = data['response'].apply(lambda x : get_bert_embedding(x,bert))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rst8TrSqLD6y"
      },
      "outputs": [],
      "source": [
        "response_embedding = tf.convert_to_tensor(data['response_embedding'].tolist())\n",
        "response_embedding = tf.squeeze(response_embedding, axis=1)\n",
        "\n",
        "positive_embedding = tf.convert_to_tensor(data[\"positive\"])\n",
        "negative_embedding = tf.convert_to_tensor(data[\"negative\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05XWZR_wJcLr"
      },
      "outputs": [],
      "source": [
        "triplet_loss = losses.TripletLoss(model=model, distance_metric=losses.SiameseDistanceMetric.COSINE_DISTANCE, margin=0.3)\n",
        "loss_value = triplet_loss.forward(response_embedding, positive_embedding, negative_embedding)\n",
        "\n",
        "print(\"Triplet Loss:\", loss_value.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvdux9cAfN06"
      },
      "source": [
        "By including bert in the training loop, i feel like I’m drifting away from my initial goal, which is to generate a response that’s close to the positive example and far from the negative one. <br>\n",
        "\n",
        "\n",
        "It feels like I’m artificially lowering the loss, since the model is also learning how to represent those elements better <br>\n",
        "I was actually thinking of BERT more like an external evaluator, rather than something that’s truly part of the model being optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts5cq_OzOI3I"
      },
      "source": [
        "#### BERT frozen version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FKleIj1jtLF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TripletTextDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.anchors = dataframe['inputs'].tolist()\n",
        "        self.positives = dataframe['positive'].tolist()\n",
        "        self.negatives = dataframe['negative'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchors)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'anchor': self.anchors[idx],\n",
        "            'positive': self.positives[idx],\n",
        "            'negative': self.negatives[idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu2p172tGp0c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, BertModel, T5Tokenizer, T5ForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TripletTextDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.anchors = dataframe['inputs'].tolist()\n",
        "        self.positives = dataframe['positive'].tolist()\n",
        "        self.negatives = dataframe['negative'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchors)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'anchor': self.anchors[idx],\n",
        "            'positive': self.positives[idx],\n",
        "            'negative': self.negatives[idx]\n",
        "        }\n",
        "\n",
        "class CustomModel:\n",
        "    def __init__(self, truncate=True, path='/content/dataset_big_patent_v3.json',\n",
        "                 bert_model_name=\"bert-base-uncased\", margin=0.3, device=None):\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name).to(self.device)\n",
        "\n",
        "        self.t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "        self.T5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(self.device)\n",
        "\n",
        "        self.triplet_loss_fn = torch.nn.TripletMarginLoss(margin=margin, p=2)\n",
        "\n",
        "        self.load_data(path)\n",
        "        if truncate:\n",
        "            self.data[\"anchor\"] = self.data[\"anchor\"].apply(self.truncate_text)\n",
        "\n",
        "        self.data['inputs'] = 'question: ' + self.data['query'] + '\\ncontext: ' + self.data['anchor']\n",
        "        self.dataset = TripletTextDataset(self.data)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        self.data = pd.DataFrame(data)\n",
        "\n",
        "    def truncate_text(self, text):\n",
        "        text_lower = text.lower()\n",
        "        point = next((p for p in (\n",
        "            text_lower.find('technical field'),\n",
        "            text_lower.find('invention'),\n",
        "            text_lower.find('disclosure'),\n",
        "            text_lower.find('this application')\n",
        "        ) if p != -1), None)\n",
        "        return text[point:] if point is not None else text\n",
        "\n",
        "    def get_embedding(self, text, requires_grad=False):\n",
        "        tokens = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "        if requires_grad:\n",
        "            outputs = self.bert(**tokens)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert(**tokens)\n",
        "        return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
        "\n",
        "    def fit(self, optimizer, batch_size=8, epochs=10):\n",
        "        self.T5.train()\n",
        "        self.bert.eval()\n",
        "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "            for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "                input_texts = batch['anchor']\n",
        "                tokenized = self.t5_tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "                generated_ids = self.T5.generate(\n",
        "                    **tokenized,\n",
        "                    num_beams=4,\n",
        "                    max_length=256,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "                generated_texts = [self.t5_tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "\n",
        "                # Get anchor embeddings with grad\n",
        "                anchor_emb = torch.stack([self.get_embedding(text, requires_grad=True) for text in generated_texts])\n",
        "\n",
        "                # Get positive and negative embeddings without grad\n",
        "                positive_emb = torch.stack([self.get_embedding(p) for p in batch['positive']])\n",
        "                negative_emb = torch.stack([self.get_embedding(n) for n in batch['negative']])\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = self.triplet_loss_fn(anchor_emb, positive_emb, negative_emb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            mean_loss = total_loss / len(dataloader)\n",
        "            print(f\"Mean triplet loss for epoch {epoch + 1}: {mean_loss:.4f}\")\n",
        "\n",
        "        return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4xQHyqncE3c"
      },
      "outputs": [],
      "source": [
        "model = CustomModel(path='/content/dataset_big_patent_v3.json')\n",
        "optimizer = torch.optim.AdamW(model.T5.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf0cop_bvPrO"
      },
      "outputs": [],
      "source": [
        "mtorch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wrXUnW4qGsZ"
      },
      "outputs": [],
      "source": [
        "model.fit(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm7urPEzSndH"
      },
      "source": [
        "### Training T5 + BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy9FrhXZU52Y"
      },
      "source": [
        "Rq : .detach().cpu(). peut être utile pour stocker des éléments autre part que dans le gpu, et donc économiser de la mémoire pour des calculs vraiement utiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWToQCEMSsbY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TripletTextDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.anchors = dataframe['inputs'].tolist()\n",
        "        self.positives = dataframe['positive'].tolist()\n",
        "        self.negatives = dataframe['negative'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchors)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'anchor': self.anchors[idx],\n",
        "            'positive': self.positives[idx],\n",
        "            'negative': self.negatives[idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F29CeaPf67kn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5ForConditionalGeneration\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, truncate=True, path='/content/dataset_big_patent_v3.json',\n",
        "                 bert_model_name=\"bert-base-uncased\", margin=0.3, device=None):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "        self.bert = AutoModel.from_pretrained(bert_model_name).to(self.device)\n",
        "\n",
        "        self.t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "        self.T5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(self.device)\n",
        "\n",
        "        self.triplet_loss_fn = torch.nn.TripletMarginLoss(margin=margin, p=2)\n",
        "\n",
        "        self.load_data(path)\n",
        "        if truncate:\n",
        "            self.data[\"anchor\"] = self.data[\"anchor\"].apply(self.truncate_text)\n",
        "\n",
        "        self.data['inputs'] = 'question: ' + self.data['query'] + '\\ncontext: ' + self.data['anchor']\n",
        "        self.dataset = TripletTextDataset(self.data)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        self.data = pd.DataFrame(data)\n",
        "\n",
        "    def truncate_text(self, text):\n",
        "        text_lower = text.lower()\n",
        "        point = next((p for p in (\n",
        "            text_lower.find('technical field'),\n",
        "            text_lower.find('invention'),\n",
        "            text_lower.find('disclosure'),\n",
        "            text_lower.find('this application')\n",
        "        ) if p != -1), None)\n",
        "        return text[point:] if point is not None else text\n",
        "\n",
        "    def get_embedding(self, text, requires_grad=True):\n",
        "        tokens = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "        if requires_grad:\n",
        "            outputs = self.bert(**tokens)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert(**tokens)\n",
        "        return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
        "\n",
        "    def fit(self, optimizer, batch_size=32, epochs=20):\n",
        "        self.T5.train()\n",
        "        self.bert.train()  # Enable training for BERT\n",
        "\n",
        "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "            for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "                input_texts = batch['anchor']\n",
        "                tokenized = self.t5_tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "                generated_ids = self.T5.generate(\n",
        "                    **tokenized,\n",
        "                    num_beams=4,\n",
        "                    max_length=512,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "                generated_texts = [self.t5_tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "\n",
        "                # Get anchor embeddings with grad\n",
        "                anchor_emb = torch.stack([self.get_embedding(text, requires_grad=True) for text in generated_texts])\n",
        "\n",
        "                # Get positive and negative embeddings without grad\n",
        "                positive_emb = torch.stack([self.get_embedding(p) for p in batch['positive']])\n",
        "                negative_emb = torch.stack([self.get_embedding(n) for n in batch['negative']])\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = self.triplet_loss_fn(anchor_emb, positive_emb, negative_emb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "\n",
        "                del tokenized, generated_ids, generated_texts\n",
        "                del anchor_emb, positive_emb, negative_emb, loss\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "            mean_loss = total_loss / len(dataloader)\n",
        "            print(f\"Mean triplet loss for epoch {epoch + 1}: {mean_loss:.4f}\")\n",
        "\n",
        "        return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI8w8w2iToJX"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "model = CustomModel()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "model.fit(optimizer, batch_size=12, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC5fcTmTWuWB"
      },
      "outputs": [],
      "source": [
        "'''torch.save(model.T5.state_dict(), \"t5_checkpoint_similarity_4elmts.pt\")\n",
        "torch.save({\n",
        "    'epoch': 10,\n",
        "    'model_state_dict': model.T5.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}, 't5_checkpoint_full_similarity_4elmts.pt')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U1XpMW3VL8w"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory used: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlRxkjcygF0K"
      },
      "source": [
        "Evaluate it (on the train dataset too ... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSRiCILVgIxt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, batch_size=16, epochs=1):\n",
        "    model.T5.eval()\n",
        "    model.bert.eval()\n",
        "    model.to(model.device)\n",
        "\n",
        "    dataloader = DataLoader(model.dataset, batch_size=batch_size, shuffle=True)\n",
        "    correct = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "            with torch.no_grad():\n",
        "                input_texts = batch['anchor']\n",
        "                tokenized = model.t5_tokenizer(\n",
        "                    input_texts,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True\n",
        "                ).to(model.device)\n",
        "\n",
        "                generated_ids = model.T5.generate(\n",
        "                    **tokenized,\n",
        "                    num_beams=4,\n",
        "                    max_length=512,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "                generated_texts = [model.t5_tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "\n",
        "                anchor_emb = torch.stack([\n",
        "                    model.get_embedding(text).to(model.device)\n",
        "                    for text in generated_texts\n",
        "                ])\n",
        "\n",
        "                positive_emb = torch.stack([\n",
        "                    model.get_embedding(p).to(model.device)\n",
        "                    for p in batch['positive']\n",
        "                ])\n",
        "                negative_emb = torch.stack([\n",
        "                    model.get_embedding(n).to(model.device)\n",
        "                    for n in batch['negative']\n",
        "                ])\n",
        "\n",
        "                # cosine_similarity shape: [batch_size]\n",
        "                sim_pos = torch.nn.functional.cosine_similarity(anchor_emb, positive_emb, dim=1)\n",
        "                sim_neg = torch.nn.functional.cosine_similarity(anchor_emb, negative_emb, dim=1)\n",
        "\n",
        "                correct += (sim_pos > sim_neg).sum().item()\n",
        "\n",
        "                del tokenized, generated_ids, generated_texts\n",
        "                del anchor_emb, positive_emb, negative_emb\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return correct / len(model.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ctH5G9jjTh4"
      },
      "outputs": [],
      "source": [
        "evaluate(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5RWGY4yrEZ9"
      },
      "outputs": [],
      "source": [
        "!pip install nbstripout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0VCCsfDrKxb"
      },
      "outputs": [],
      "source": [
        "!nbstripout similarity.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEhd5ojCrauW"
      },
      "outputs": [],
      "source": [
        "!ls ../content"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4r_8IpdczCMp"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}